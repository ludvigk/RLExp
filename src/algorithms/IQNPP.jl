export IQNPPLearner, ImplicitQuantileNetPP

using Functors: @functor
using Flux: params, unsqueeze
using Random: AbstractRNG, GLOBAL_RNG
using StatsBase: mean
using StatsFuns
using Zygote: gradient
using ChainRulesCore: ignore_derivatives
using RLExp

"""
    ImplicitQuantileNet(;œà, œï, header)
```
        quantiles (n_action, n_quantiles, batch_size)
           ‚Üë
         header
           ‚Üë
feature ‚Ü±  ‚®Ä   ‚Ü∞ transformed embedding
       œà       œï
       ‚Üë       ‚Üë
       s        œÑ
```
"""
Base.@kwdef struct ImplicitQuantileNetPP{A,B,C}
    œà::A
    œï::B
    header::C
end

@functor ImplicitQuantileNetPP

function (net::ImplicitQuantileNetPP)(s, emb)
    features = net.œà(s)  # (n_feature, batch_size)
    emb_aligned = net.œï(emb)  # (n_feature, N * batch_size)
    merged = unsqueeze(features, dims=2) .* reshape(emb_aligned, size(features, 1), :, size(features, 2))  # (n_feature, N, batch_size)
    quantiles = net.header(reshape(merged, size(merged)[1:end-2]..., :)) # flattern last two dimension first
    reshape(quantiles, :, size(merged, 2), size(merged, 3))  # (n_action, N, batch_size)
end

Base.@kwdef mutable struct IQNPPLearner{A<:Approximator{<:TwinNetwork}} <: AbstractLearner
    approximator::A
    Œ≥::Float32 = 0.99f0
    N::Int = 32
    N‚Ä≤::Int = 32
    N‚Çë‚Çò::Int = 64
    K::Int = 32
    rng::AbstractRNG = GLOBAL_RNG
    device_rng::AbstractRNG = rng
    # for logging
    loss::Float32 = 0.0f0
    logging_params = DefaultDict(0.0)
end

@functor IQNPPLearner (approximator, device_rng)

embed(x, N‚Çë‚Çò) = cos.(Float32(œÄ) .* (1:N‚Çë‚Çò) .* reshape(x, 1, :))

# the last dimension is batch_size
function (learner::IQNPPLearner)(s::AbstractArray)
    batch_size = size(s)[end]
    œÑ = rand(learner.device_rng, Float32, learner.K, batch_size)

    œÑ‚Çë‚Çò = embed(œÑ, learner.N‚Çë‚Çò)
    quantiles = learner.approximator(s, œÑ‚Çë‚Çò)
    dropdims(mean(quantiles; dims=2); dims=2)
end

function (L::IQNPPLearner)(env::AbstractEnv)
    s = env |> state |> send_to_device(L)
    q = s |> unsqueeze(dims=ndims(s) + 1) |> L |> vec
    q |> send_to_host
end

function huber_norm(td, Œ∫)
    abs_error = abs.(td)
    quadratic = min.(abs_error, Œ∫)
    linear = abs_error .- quadratic
    return 0.5f0 .* quadratic .* quadratic .+ Œ∫ .* linear
end

function l2_norm(td)
    return td .^ 2
end

function l1_norm(td)
    return abs.(td)
end

function ludde_norm(td)
    d = abs.(td)
    return (d .- 1) .* d ./ log.(d .+ 1f8)
end

cauchy_norm(td) = log.(1 .+ abs.(td) .^ 2) 
function lol_norm(td)
    d = abs.(td)
    return d .* log.(1 .+ d)
end

function lol2_norm(td)
    # d = abs.(td)
    return sqrt.(1 .+ td .^ 2)
end

function mix_norm(td)
    d = reshape(td .^ 2, 1, :)
    h = gpu(1 ./ reshape(collect(1:10), :, 1))
    # s = zero(td)
    # reshape(td, )
    # for l=1:10
    #    s = s + exp.(-d ./ l) 
    # end
    reshape(sum(h * d, dims=1), size(td)...)
end

hhh = gpu(1 ./ reshape([0.01, 0.1, 0.5, 1, 5, 10, 20, 50], :, 1))

function rqn(td)
    d = reshape(td .^ 2, 1, :)
    # h = gpu(1 ./ reshape([0.01, 0.1, 0.5, 1, 5, 10, 20, 50], :, 1))
    a = 0.5
    return -1 ./ sqrt.(1 .+ hhh * d)
end

function energy_distance(x, y)
    # n = size(x, 2)
    # m = size(y, 2)
    # œÑ‚Çë‚Çò_ = Flux.unsqueeze(x, dims=2)
    # _œÑ‚Çë‚Çò = Flux.unsqueeze(x, dims=3)
    # _œÑ‚Çë‚Çò‚Ä≤ = Flux.unsqueeze(x, dims=3)

    # œµ1 = Zygote.@ignore (œÑ‚Çë‚Çò_ .- _œÑ‚Çë‚Çò) .^ 2
    # œµ2 = Zygote.@ignore (œÑ‚Çë‚Çò_ .- _œÑ‚Çë‚Çò‚Ä≤) .^ 2

    x_ = Flux.unsqueeze(x, dims=2)
    _x = Flux.unsqueeze(x, dims=3)
    _y = Flux.unsqueeze(y, dims=3)
    # y_ = Flux.unsqueeze(y, dims=2)
    # d_xy = dropdims(sum(lol_norm(x_ .- _y), dims=(2, 3)), dims=(2, 3))
    d_xy = rqn(x_ .- _y)
    # d_xx = dropdims(sum(lol_norm(x_ .- _x), dims=(2, 3)), dims=(2, 3))
    d_xx = rqn(x_ .- _x)
    # d_yy = mix_norm(y_ .- _y)
    # d_xx = dropdims(sum(l1_norm(y_ .- _y), dims=(2, 3)), dims=(2, 3))
    Œµ = 2mean(d_xy, dims=(2,3)) .- mean(d_xx, dims=(2,3)) #.- mean(d_yy, dims=(2,3))
    return Œµ
end

# function energy_distance_fast(x, y; Œ∫=1.0f0)
#     n = size(x, 2)
#     m = size(y, 2)

#     x_ = permutedims(x, (2, 1, 3))
#     y_ = permutedims(y, (2, 1, 3))
#     x_ = Flux.unsqueeze(x, dims=1)
#     _x = Flux.unsqueeze(x, dims=2)
#     _y = Flux.unsqueeze(y, dims=2)
#     # d_xy = dropdims(sum(l2_norm(x_ .- _y, Œ∫), dims=(2, 3)), dims=(2, 3))
#     # d_xx = dropdims(sum(l2_norm(x_ .- _x, Œ∫), dims=(2, 3)), dims=(2, 3))
#     Œµ = dropdims(batched_mul(x_, _x)) + 2batched_mul(x_, _y)
#     # Œµ = 2 / (n * m) .* d_xy .- 1 / n^2 .* d_xx
#     return Œµ
# end

function RLBase.optimise!(learner::IQNPPLearner, batch::NamedTuple)
    A = learner.approximator
    Z = A.model.source
    Z‚Çú = A.model.target
    N = learner.N
    N‚Ä≤ = learner.N‚Ä≤
    N‚Çë‚Çò = learner.N‚Çë‚Çò
    D = device(Z)
    # s, s‚Ä≤, a, r, t = map(x -> batch[x], SS‚Ä≤ART)
    s = DenseCuArray(batch.state)
    # s = send_to_device(D, collect(batch.state))
    r = send_to_device(D, batch.reward)
    t = send_to_device(D, batch.terminal)
    s‚Ä≤ = DenseCuArray(batch.next_state)
    # s‚Ä≤ = send_to_device(D, collect(batch.next_state))
    a = send_to_device(D, batch.action)


    batch_size = length(t)
    œÑ‚Ä≤ = rand(learner.device_rng, Float32, N‚Ä≤, batch_size)  # TODO: support Œ≤ distribution
    œÑ‚Çë‚Çò‚Ä≤ = embed(œÑ‚Ä≤, N‚Çë‚Çò)
    z‚Çú = Z‚Çú(s‚Ä≤, œÑ‚Çë‚Çò‚Ä≤)
    avg_z‚Çú = mean(z‚Çú, dims=2)

    if haskey(batch, :next_legal_actions_mask)
        masked_value = similar(batch.next_legal_actions_mask, Float32)
        masked_value = fill!(masked_value, typemin(Float32))
        masked_value[batch.next_legal_actions_mask] .= 0
        avg_z‚Çú .+= masked_value
    end

    a‚Çú = argmax(avg_z‚Çú, dims=1)
    a‚Çú = a‚Çú .+ typeof(a‚Çú)(CartesianIndices((0:0, 0:N‚Ä≤-1, 0:0)))
    q‚Çú = reshape(z‚Çú[a‚Çú], :, batch_size)
    target = reshape(r, 1, batch_size) .+ learner.Œ≥ * reshape(1 .- t, 1, batch_size) .* q‚Çú  # reshape to allow broadcast

    œÑ = rand(learner.device_rng, Float32, N, batch_size)
    œÑ‚Çë‚Çò = embed(œÑ, N‚Çë‚Çò)
    a = CartesianIndex.(repeat(a, inner=N), 1:(N*batch_size))

    gs = gradient(params(A)) do
        z_raw = Z(s, œÑ‚Çë‚Çò)
        # z_raw = Z(s, œÑ‚Çë‚Çò‚Ä≤)
        z = reshape(z_raw, size(z_raw)[1:end-2]..., :)
        q = z[a]

        target = reshape(target, 1, N‚Ä≤, batch_size)
        q = reshape(q, 1, N, batch_size)
        loss_batch = energy_distance(q, target)
        # loss = loss_batch .* œÑ‚Çë‚Çò
        loss = mean(loss_batch)
        ignore_derivatives() do
            learner.loss = loss
            learner.logging_params["ùêø"] = loss
        end
        loss
    end

    optimise!(A, gs)
end