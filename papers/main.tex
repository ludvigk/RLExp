% \documentclass{uai2021} % for initial submission
\documentclass[]{uai2021} % after acceptance, for a revised
                                    % version; also before submission to
                                    % see how the non-anonymous paper
                                    % would look like
%% There is a class option to choose the math font
% \documentclass[mathfont=ptmx]{uai2021} % ptmx math instead of Computer
                                         % Modern (has noticable issues)
% \documentclass[mathfont=newtx]{uai2021} % newtx fonts (improves upon
                                          % ptmx; less tested, no support)
% NOTE: Only keep *one* line above as appropriate, as it will be replaced
%       automatically for papers to be published. Do not make any other
%       change above this note for an accepted version.

%% Choose your variant of English; be consistent
\usepackage[american]{babel}
% \usepackage[british]{babel}

%% Some suggested packages, as needed:
\usepackage{natbib} % has a nice set of citation styles and commands
    \bibliographystyle{plainnat}
    \renewcommand{\bibsection}{\subsubsection*{References}}
\usepackage{mathtools} % amsmath with fixes and additions
% \usepackage{siunitx} % for proper typesetting of numbers and units
\usepackage{booktabs} % commands to create good-looking tables
\usepackage{tikz} % nice language for creating drawings and diagrams
\usepackage{amssymb}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{bm}

%% Provided macros
% \smaller: Because the class footnote size is essentially LaTeX's \small,
%           redefining \footnotesize, we provide the original \footnotesize
%           using this macro.
%           (Use only sparingly, e.g., in drawings, as it is quite small.)

%% Self-defined macros
\newcommand{\swap}[3][-]{#3#1#2} % just an example
\newcommand{\defeq}{\vcentcolon=}
\newcommand{\R}{\mathbb{R}}
\newcommand{\E}{\mathbb{E}}
\newcommand{\N}{\mathbb{N}}
\newcommand{\D}{\mathcal{D}}
\newcommand{\B}{\mathcal{B}}
\newcommand{\X}{\mathbf{X}}
\newcommand{\f}{\mathbf{f}}
\newcommand{\state}{\mathcal{S}}
\newcommand{\action}{\mathcal{A}}
\newcommand{\KL}{\mathrm{KL}}

\title{}

% The standard author block has changed for UAI 2021 to provide
% more space for long author lists and allow for complex affiliations
%
% All author information is authomatically removed by the class for the
% anonymous submission version of your paper, so you can already add your
% information below.
%
% Add authors
\author[1]{\href{mailto:<jj@example.edu>?Subject=LALAL}{Author}{}}
  
\begin{document}
\maketitle

\begin{abstract}
%   This is the abstract for this article.
%   It should give a self-contained single-paragraph summary of the article's contents, including context, results, and conclusions.
%   Avoid citations; but if you do, you must give essentially the whole reference.
%   For example: This whole paper is devoted to praising É. Š. Åland von Vèreweg's most recent book (“Utopia's government formation problems during the last millenium”, Springevier Publishers, 2016).
%   Also, do not put mathematical notation and abbreviations in your abstract; be descriptive.
%   So not “we solve \(x^2+A xy+y^2\), where \(A\) is an RV”, but “we solve quadratic equations in two unknowns in which a single coefficient is a random variable”.
%   The reason is that mathematical notation will not display correctly when the abstract is reused on the proceedings website, for example, and that one should not assume the abstract's reader knows the abbreviation.
%   Of course the same remarks hold for your paper's title.
\end{abstract}

% \section{Introduction}
% Bayesian reinforcement learning is an approach to RL that expresses uncertainty in the
% Markov decision process (MDP) via a posterior distribution \citep{ghavamzadeh_bayesian_2015}.
% The posterior captures the uncertainty in transition and reward distributions.

\section{Background}
A reinforcement learning environement is modelled as a Markov decision process (MDP)
\(M = \langle \state, \action, r, P, P_0, \gamma \rangle\), where \(\state\) is the
state space and \(\action\) is the set of available actions. At time \(t=0\) a state
\(s_0\) is sampled from the distribution \(P_0(\cdot)\). At each timestep an action
\(a_t \sim \pi(\cdot \vert s_t)\) is selected and the agent transitions to a new state
state \(s_{t+1} \sim P(\cdot \vert s_t, a_t)\). A scalar reward
\(r_{t+1} = r(\cdot \vert s_{t+1}, s_t, a_t)\) is observed. As the agent and environement
interact in a sequence of time steps, a history of observations
\(\mathcal{H}_t = (s_0, a_0, s_1, r_1, a_1, s_2, r_2, \dots, s_t, r_t)\) is collected.
The goal is to find a policy \(\pi^\star\), such that sampling actions
\(a \sim \pi^*(\cdot \vert s)\) maximizes the expected future reward,
\(J^\pi \defeq \E_\pi \left[ \sum_{t=0}^\infty \gamma^t r_t \right]\). An efficient
agent must be able to learn from the data it collects, but since the data is
dependent on the policy, it must also prioritize to explore states and actions that
the agent can learn a lot from.

The \(Q\)-function, \(Q^\pi(s,a)\), is defined as the expected reward of taking action
\(a\) in state \(s\) and then following policy \(\pi\) thereafter:
\(Q^\pi \defeq \E_\pi \left[ \sum_{t=0}^\infty \gamma^t r_t \vert s_0=0, a_0=0 \right]\).
The Bellman operator on \(Q^\pi\) is defined as
\(\B[Q^\pi](s_t,a_t) \defeq \E_{P(s_{t+1} \vert s_t, a_t)\pi(a_{t+1} \vert s+t)}
\left[ r(s_{t+1}, s_t, a) + \gamma Q^\pi(s_{t+1}, a_{t+1}) \right]\). With the
\(Q\)-function we can define a policy \(\pi(a \vert s) = \arg\max_a Q(s,a)\).
In deep RL, we model \(Q\) with a neural network \(\hat{Q}_\omega\).
One way to facilitate exploration in this policy is to introduce uncertainty in the
\(Q\)-value.
\citet{fortunato_noisy_2019} introduced noisy networks for exploration.
These are networks with stochastic weights, where each weight \(w^{(l)}_{ij}\)
has an added pertrubaition sampled from a noise distribution with standard deviation \(\sigma^{(l)}_{ij}\).
After initializing \(\bm{\mu}\) and \(\bm{\sigma}\) such that the network has sufficient
stochasticity for exploration, both parameters are learned using standard backpropagation.
The approach is similar to variational inference schemes such as bayes by backprop~\cite{blundell_weight_2015} where the weights are normally distributed with mean 
\(\mu^{(l)}_{ij}\) and standard deviation \(\sigma^{(l)}_{ij}\),


\cite{fellows_bayesian_2021} does this by defining the Bayesian Bellman
operator (BBO). They define \(P_B(b \vert s, a, \omega)\) as the distribution over
Bellman functions such that for a noisy sample \(b_i \sim P_B(b \vert s,a,\omega)\),
\(b_i = \B[\hat{Q}_\omega](s_i, a_i) + \eta_i\). Then we approximate this with a
distribution parameterized by \(\phi\) such that
\(P(b \vert s,a,\phi) \approx P(b \vert s,a,\omega)\).
\begin{equation}
    \B_{\omega, N}^\star(s,a) \defeq \E_{P(\phi \vert \D_\omega^N)} \left[ \hat{B}_\phi(s,a) \right].
\end{equation}
Here, \(\hat{B}_\phi(s,a) = \E_{P(b \mid s, a, \phi)}[b]\), is the conditional mean of
\(P(b \vert s, a, \phi)\), and the data \(\D_\omega^N\) is defined as the collection of
sampled \(Q\)-values, states and actions \(\D_\omega^N \defeq \{b_i, s_i, a_i\}_{i=1:N}\).
Under the assumption that each state \(s_i\) is drawn i.i.d from a distribution with
support over \(\state\), or from an ergodic Markov chain defined over a \(\sigma\)-algebra that is
countably generated from \(\state\) they find that \(\omega^\star\) such that
\(\hat{Q}_{\omega^\star} = \B^\star_{\omega^\star, N}\) can be found 
by minimising the mean squared Bayesian Bellman error (MSBBE):
\begin{equation}
    \mathrm{MSBBE}_N(\omega) \defeq \lVert \hat{Q}_{\omega} - \B^\star_{\omega, N} \lVert^2_{\rho, \pi},
\end{equation}
And that an unbiased estimate of \(\nabla_\omega \mathrm{MSBBE}_N(\omega)\) can be calculated
as long as we can sample from \(P(\phi \vert \D_\omega^N)\).

Nonlinear function approximators such as neural networks typically have an intractable
posterior, so we cannot calculate the posterior analytically. Instead we use a posterior approximation
\(q(\phi \vert \D_\omega^N) \approx P(\phi \vert \D_\omega^N)\), together with an algorithm
for approximate inference. \citet{fellows_bayesian_2021} presents the Bayesian Bellman Actor-Critic
using randomized priors \citep{osband_randomized_2018}, and show state of the art exploratory
behaviour.

For most approximate inference methods in deep learning, it is difficult to incorporate domain
knowledge into the prior. Typically the prior distribution is defined on the parameters as \(P(\phi)\),
while the knowledge exists in the function space \(P(a \vert s)\).

Functional variational Bayesian neural networks \citep{sun_functional_2019} lets us define a prior on the
\emph{function space}. In a Q-learning algorithm, this would be a function
\(f : \state \times \action \rightarrow \R\). For now we generalize, and say that
\(f : \mathcal{X} \rightarrow \mathcal{Y}\).
\citet{sun_functional_2019} show that for two stochastic processes \(P\) and \(Q\):

\begin{equation}
    \KL[P \Vert Q] = \sup_{n \in \N, \X \in \mathcal{X}^n} \KL \left[ P_\X \Vert Q_\X \right],
\end{equation}

where \(\X \in \mathcal{X}^n\) is a finite measurement set and \(P_\X, Q_\X\) are the marginal distributions
at \(\X\). They further show that if \(\f^\X\) are the function values at points \(\X\), then

\begin{multline}\label{eq:fkl}
    \nabla_\phi \KL[q_\phi(\f^X) \Vert p(\f^\X)] \\
    = \E \left[ \nabla_\phi \f^\X( \nabla_\f \log q(\f^\X) - \nabla_\f \log p(\f^\X)) \right].
\end{multline}

The difficult part in \eqref{eq:fkl} is to estimate \(\nabla_\f \log q(\f^\X)\) and \(\nabla_\f \log p(\f^\X)\).
To estimate these log-density gradients, they use a spectral Stein gradient estimator \citep{shi_spectral_2018}.
Given enough function value samples, the spectral Stein gradient estimator can estimate score functions
for both in-distribution and out-of-distribution samples. This means that we can use it to estimate
both gradients. \(\nabla_\f \log q(\f^\X)\) is likely intractable intractable, considering \(q_\phi\) is
a neural network with stochastic weights. Depending on how we define the prior, however, \(\nabla_\f \log p(\f^\X)\)
can be easy to compute analytically. To reduce variance in the gradients we aim to define tractable priors.

% \section{Background}
% \citet{fellows_bayesian_2021} defines the Bayesian Bellman operator (BBO) as:
% \begin{equation}
%     \B_{\omega, N}^\star(s,a) \defeq \E_{P(\phi \vert \D_\omega^N)} \left[ \hat{B}_\phi(s,a) \right],
% \end{equation}
% where \(\hat{B}_\phi(s,a) = \E_{P(b \mid s, a, \phi)}[b]\), is the conditional mean of the
% parametric distribution \(P(b \vert s, a, \phi)\), which is a model of the true
% data-generating distribution \(P(b \vert s, a, \omega)\). They further show that we can
% find \(\omega^\star\) such that \(\hat{Q}_{\omega^\star} = \B^\star_{\omega^\star, N}\)
% by minimising the mean squared Bayesian Bellman error (MSBBE):

% \begin{equation}
%     \mathrm{MSBBE}_N(\omega) \defeq \lVert \hat{Q}_{\omega} - \B^\star_{\omega, N} \lVert^2_{\rho, \pi}
% \end{equation}
% An unbiased estimate of \(\nabla_\omega \mathrm{MSBBE}_N(\omega)\) can be calculated as
% long as we can sample from \(P(\phi \vert \D_\omega^N)\). 

% Nonlinear function approximators such as neural networks typically have an intractable
% posterior, so we cannot calculate the posterior analytically. Instead we use a posterior approximation
% \(q(\phi \vert \D_\omega^N) \approx P(\phi \vert \D_\omega^N)\), together with an algorithm
% for approximate inference. \citet{fellows_bayesian_2021} presents the Bayesian Bellman Actor-Critic
% using randomized priors \citep{osband_randomized_2018}, and show state of the art exploratory
% behaviour.

% For most approximate inference methods in deep learning, it is difficult to incorporate domain
% knowledge into the prior. Typically the prior distribution is defined on the parameters as \(P(\phi)\),
% while the knowledge exists in the function space \(P(a \vert s)\).

% Functional variational Bayesian neural networks \citep{sun_functional_2019} lets us define a prior on the
% \emph{function space}. In a Q-learning algorithm, this would be a function
% \(f : \state \times \action \rightarrow \R\). For now we generalize, and say that
% \(f : \mathcal{X} \rightarrow \mathcal{Y}\).
% \citet{sun_functional_2019} show that for two stochastic processes \(P\) and \(Q\):

% \begin{equation}
%     \KL[P \Vert Q] = \sup_{n \in \N, \X \in \mathcal{X}^n} \KL \left[ P_\X \Vert Q_\X \right],
% \end{equation}

% where \(\X \in \mathcal{X}^n\) is a finite measurement set and \(P_\X, Q_\X\) are the marginal distributions
% at \(\X\). They further show that if \(\f^\X\) are the function values at points \(\X\), then

% \begin{multline}\label{eq:fkl}
%     \nabla_\phi \KL[q_\phi(\f^X) \Vert p(\f^\X)] \\
%     = \E \left[ \nabla_\phi \f^\X( \nabla_\f \log q(\f^\X) - \nabla_\f \log p(\f^\X)) \right].
% \end{multline}

% The difficult part in \eqref{eq:fkl} is to estimate \(\nabla_\f \log q(\f^\X)\) and \(\nabla_\f \log p(\f^\X)\).
% To estimate these log-density gradients, they use a spectral Stein gradient estimator \citep{shi_spectral_2018}.
% Given enough function value samples, the spectral Stein gradient estimator can estimate score functions
% for both in-distribution and out-of-distribution samples. This means that we can use it to estimate
% both gradients. \(\nabla_\f \log q(\f^\X)\) is likely intractable intractable, considering \(q_\phi\) is
% a neural network with stochastic weights. Depending on how we define the prior, however, \(\nabla_\f \log p(\f^\X)\)
% can be easy to compute analytically. To reduce variance in the gradients we aim to define tractable priors.

\section{Method}
Instead of a prior on the parameter space, we would like to have a prior on the \(Q\)-function space.
Using the functional variational neural network framework explored earlier, our maximization target
becomes:

\begin{equation}
    \log p(\D_\omega^N \vert f) - \KL \left[ q(\f^{\D_\omega^N}, \f^M) \Vert p(\f^{\D_\omega^N}, \f^M) \right],
\end{equation}
where \(\f^{\D_\omega^N}\) is \(f\) applied to the dataset \(\D_\omega^N\), and \(\f^M\) is \(f\) applied to
a set of i.i.d. random points \(M = \{m_i \sim c \mid i=1,\dots,k\} \subseteq \state \times \action\)
with full support. We will let

\begin{equation}
    M = \{(s_i, a_j) \; \forall s_i \in \D_\omega^N,\, \forall a_j \in \action\}. 
\end{equation}
To justify this we need to show that \(supp(c) = \state \times \action\).
Under the same assumption about the distribution of each state \(s_i\) from BBO,
\(supp(c) = \state \times \action\) follows trivially.

Since we are modeling \(q_\phi\) with a Bayesian neural network, and \(Q_\omega\) with a neural network,
we have a bilevel optimization problem. To solve this, we employ a similar strategy to \cite{fellows_bayesian_2021},
where we have a two-timescale gradient update. \(\phi\) and \(\omega\) are updated using stochastic
gradient descent at different timescales to ensure stable convergence.

% We can now define a prior \(P(f)\), however, this prior exists in the space
% \(P(Q \vert s, a) : (\state \times \action \rightarrow \R) \rightarrow \R\), which can be hard to
% define. We would like the prior to be of the form \(P(a \vert s)\).

\begin{algorithm}
\caption{Functional Bayesian RL}
\begin{algorithmic}
    \State \(\D \gets \emptyset\)
    \State \(s \sim P_0\)
    \State Initialize \(\phi, \omega\)
    \While{not converged}
        \State \(f \sim q_\phi\)
        \State \(a \sim \arg\max_a f(s,a)\)
        \State \(s' \sim P(\cdot \vert s,a)\)
        \State \(r = r(s',a,s)\)
        \State \(\D \gets \D \cup \{s,a,r,s'\}\)
        \State \Call{UpdatePosterior}{$\phi, \omega, \D$}
    \EndWhile
    \Statex
    \Function{UpdatePosterior}{$\phi, \omega, \D$}
        \While{not converged}
            \State \(T \sim \D\)
            \State \(\f \gets \{f_i \sim q_\phi \text{ for } i=1,\dots,k\}\)
            \State \(\mathcal{F} \gets \emptyset\)
            \State \(\Delta_\mathcal{L} \gets 0\)
            \For{\(\{s,a,r,s'\} \in T\)}
                \State \(\bm{\hat{a}} \gets \arg\max_a \f(s,a)\)
                \State \(\mathcal{F} \gets \mathcal{F} \cup \f(s,a)\)
                \State \(G \gets r + \gamma Q_\omega(s,a)\)
                \State \(\Delta_\mathcal{L} \gets \Delta_\mathcal{L} - \frac{1}{k} \frac{1}{\lvert T \rvert} \nabla_\phi \log p(\bm{\hat{a}} \vert G)\)
            \EndFor
            \State \(\Delta_\KL \gets SSGE(p, \mathcal{F})\)
            \State \(\phi \gets \phi + \eta_\phi (\Delta_\mathcal{L} - \Delta_\KL)\)
            \State \(\hat{B}_\phi \gets \frac{1}{k}\frac{1}{\lvert T \rvert}\sum_{i=1}^k \sum_{b \in \mathcal{F}} b\)
            \State \(\Delta_\omega \gets \sum_{\{s,a,r,s'\} \in T} \nabla_\omega \lVert \hat{B}_\phi(s,a) - Q_\omega(s,a) \rVert^2_2 \)
            \State \(\omega \gets \omega - \eta_\omega \Delta_\omega\)
        \EndWhile
    \EndFunction
\end{algorithmic}
\end{algorithm}


\begin{acknowledgements} % will be removed in pdf for initial submission,
                         % so you can already fill it to test with the
                         % ‘accepted’ class option
    Briefly acknowledge people and organizations here.

    \emph{All} acknowledgements go in this section.
\end{acknowledgements}

\bibliography{uai2021-template}

\appendix
% NOTE: necessary when ptmx or no mathfont class option is given
% \providecommand{\upGamma}{\Gamma}
% \providecommand{\uppi}{\pi}
% \section{Math font exposition}
% How math looks in equations is important:
% \begin{equation*}
%   F_{\alpha,\beta}^\eta(z) = \upGamma(\tfrac{3}{2}) \prod_{\ell=1}^\infty\eta \frac{z^\ell}{\ell} + \frac{1}{2\uppi}\int_{-\infty}^z\alpha \sum_{k=1}^\infty x^{\beta k}\mathrm{d}x.
% \end{equation*}
% However, one should not ignore how well math mixes with text:
% The frobble function \(f\) transforms zabbies \(z\) into yannies \(y\).
% It is a polynomial \(f(z)=\alpha z + \beta z^2\), where \(-n<\alpha<\beta/n\leq\gamma\), with \(\gamma\) a positive real number.

\end{document}
