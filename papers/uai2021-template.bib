
@inproceedings{blundell_weight_2015,
	address = {Lille, France},
	series = {Proceedings of {Machine} {Learning} {Research}},
	title = {Weight {Uncertainty} in {Neural} {Network}},
	volume = {37},
	url = {http://proceedings.mlr.press/v37/blundell15.html},
	abstract = {We introduce a new, efficient, principled and backpropagation-compatible algorithm for learning a probability distribution on the weights of a neural network, called Bayes by Backprop. It regularises the weights by minimising a compression cost, known as the variational free energy or the expected lower bound on the marginal likelihood. We show that this principled kind of regularisation yields comparable performance to dropout on MNIST classification. We then demonstrate how the learnt uncertainty in the weights can be used to improve generalisation in non-linear regression problems, and how this weight uncertainty can be used to drive the exploration-exploitation trade-off in reinforcement learning.},
	booktitle = {Proceedings of the 32nd {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Blundell, Charles and Cornebise, Julien and Kavukcuoglu, Koray and Wierstra, Daan},
	editor = {Bach, Francis and Blei, David},
	month = jul,
	year = {2015},
	pages = {1613--1622},
}

@article{sun_functional_2019,
	title = {Functional {Variational} {Bayesian} {Neural} {Networks}},
	url = {http://arxiv.org/abs/1903.05779},
	abstract = {Variational Bayesian neural networks (BNNs) perform variational inference over weights, but it is difficult to specify meaningful priors and approximate posteriors in a high-dimensional weight space. We introduce functional variational Bayesian neural networks (fBNNs), which maximize an Evidence Lower BOund (ELBO) defined directly on stochastic processes, i.e. distributions over functions. We prove that the KL divergence between stochastic processes equals the supremum of marginal KL divergences over all finite sets of inputs. Based on this, we introduce a practical training objective which approximates the functional ELBO using finite measurement sets and the spectral Stein gradient estimator. With fBNNs, we can specify priors entailing rich structures, including Gaussian processes and implicit stochastic processes. Empirically, we find fBNNs extrapolate well using various structured priors, provide reliable uncertainty estimates, and scale to large datasets.},
	urldate = {2021-06-27},
	journal = {arXiv:1903.05779 [cs, stat]},
	author = {Sun, Shengyang and Zhang, Guodong and Shi, Jiaxin and Grosse, Roger},
	month = mar,
	year = {2019},
	note = {arXiv: 1903.05779},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv.org Snapshot:/home/ludvig/Zotero/storage/WHPGS7US/1903.html:text/html;arXiv Fulltext PDF:/home/ludvig/Zotero/storage/7JMKZBL9/Sun et al. - 2019 - Functional Variational Bayesian Neural Networks.pdf:application/pdf},
}

@inproceedings{osband_more_2013,
	title = {({More}) {Efficient} {Reinforcement} {Learning} via {Posterior} {Sampling}},
	volume = {26},
	url = {https://proceedings.neurips.cc/paper/2013/hash/6a5889bb0190d0211a991f47bb19a777-Abstract.html},
	urldate = {2021-10-28},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Osband, Ian and Russo, Daniel and Van Roy, Benjamin},
	year = {2013},
	file = {Full Text PDF:/home/ludvig/Zotero/storage/PGSG2ZT4/Osband et al. - 2013 - (More) Efficient Reinforcement Learning via Poster.pdf:application/pdf},
}

@article{odonoghue_uncertainty_2018,
	title = {The {Uncertainty} {Bellman} {Equation} and {Exploration}},
	url = {http://arxiv.org/abs/1709.05380},
	abstract = {We consider the exploration/exploitation problem in reinforcement learning. For exploitation, it is well known that the Bellman equation connects the value at any time-step to the expected value at subsequent time-steps. In this paper we consider a similar uncertainty Bellman equation (UBE), which connects the uncertainty at any time-step to the expected uncertainties at subsequent time-steps, thereby extending the potential exploratory beneﬁt of a policy beyond individual time-steps. We prove that the unique ﬁxed point of the UBE yields an upper bound on the variance of the posterior distribution of the Q-values induced by any policy. This bound can be much tighter than traditional count-based bonuses that compound standard deviation rather than variance. Importantly, and unlike several existing approaches to optimism, this method scales naturally to large systems with complex generalization. Substituting our UBE-exploration strategy for -greedy improves DQN performance on 51 out of 57 games in the Atari suite.},
	language = {en},
	urldate = {2021-10-28},
	journal = {arXiv:1709.05380 [cs, math, stat]},
	author = {O'Donoghue, Brendan and Osband, Ian and Munos, Remi and Mnih, Volodymyr},
	month = oct,
	year = {2018},
	note = {arXiv: 1709.05380},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Artificial Intelligence, Mathematics - Optimization and Control},
	file = {O'Donoghue et al. - 2018 - The Uncertainty Bellman Equation and Exploration.pdf:/home/ludvig/Zotero/storage/5MWSY3TP/O'Donoghue et al. - 2018 - The Uncertainty Bellman Equation and Exploration.pdf:application/pdf},
}

@article{dearden_bayesian_nodate,
	title = {Bayesian {Q}-{Learning}},
	abstract = {A central problem in learning in complex environments is balancing exploration of untested actions against exploitation of actions that are known to be good. The beneﬁt of exploration can be estimated using the classical notion of Value of Information—the expected improvement in future decision quality that might arise from the information acquired by exploration. Estimating this quantity requires an assessment of the agent’s uncertainty about its current value estimates for states. In this paper, we adopt a Bayesian approach to maintaining this uncertain information. We extend Watkins’ Q-learning by maintaining and propagating probability distributions over the Q-values. These distributions are used to compute a myopic approximation to the value of information for each action and hence to select the action that best balances exploration and exploitation. We establish the convergence properties of our algorithm and show experimentally that it can exhibit substantial improvements over other well-known model-free exploration strategies.},
	language = {en},
	author = {Dearden, Richard and Friedman, Nir and Russell, Stuart},
	pages = {8},
	file = {Dearden et al. - Bayesian Q-Learning.pdf:/home/ludvig/Zotero/storage/7NGS3ZD7/Dearden et al. - Bayesian Q-Learning.pdf:application/pdf},
}

@article{fellows_bayesian_2021,
	title = {Bayesian {Bellman} {Operators}},
	url = {http://arxiv.org/abs/2106.05012},
	abstract = {We introduce a novel perspective on Bayesian reinforcement learning (RL); whereas existing approaches infer a posterior over the transition distribution or Q-function, we characterise the uncertainty in the Bellman operator. Our Bayesian Bellman operator (BBO) framework is motivated by the insight that when bootstrapping is introduced, model-free approaches actually infer a posterior over Bellman operators, not value functions. In this paper, we use BBO to provide a rigorous theoretical analysis of model-free Bayesian RL to better understand its relationship to established frequentist RL methodologies. We prove that Bayesian solutions are consistent with frequentist RL solutions, even when approximate inference is used, and derive conditions for which convergence properties hold. Empirically, we demonstrate that algorithms derived from the BBO framework have sophisticated deep exploration properties that enable them to solve continuous control tasks at which state-of-the-art regularised actor-critic algorithms fail catastrophically.},
	language = {en},
	urldate = {2021-10-28},
	journal = {arXiv:2106.05012 [cs]},
	author = {Fellows, Matthew and Hartikainen, Kristian and Whiteson, Shimon},
	month = jun,
	year = {2021},
	note = {arXiv: 2106.05012},
	keywords = {Computer Science - Machine Learning},
	file = {Fellows et al. - 2021 - Bayesian Bellman Operators.pdf:/home/ludvig/Zotero/storage/874CC8RB/Fellows et al. - 2021 - Bayesian Bellman Operators.pdf:application/pdf},
}

@article{riquelme_deep_2018,
	title = {{DEEP} {BAYESIAN} {BANDITS} {SHOWDOWN}},
	abstract = {Recent advances in deep reinforcement learning have made signiﬁcant strides in performance on applications such as Go and Atari games. However, developing practical methods to balance exploration and exploitation in complex domains remains largely unsolved. Thompson Sampling and its extension to reinforcement learning provide an elegant approach to exploration that only requires access to posterior samples of the model. At the same time, advances in approximate Bayesian methods have made posterior approximation for ﬂexible neural network models practical. Thus, it is attractive to consider approximate Bayesian neural networks in a Thompson Sampling framework. To understand the impact of using an approximate posterior on Thompson Sampling, we benchmark well-established and recently developed methods for approximate posterior sampling combined with Thompson Sampling over a series of contextual bandit problems. We found that many approaches that have been successful in the supervised learning setting underperformed in the sequential decision-making scenario. In particular, we highlight the challenge of adapting slowly converging uncertainty estimates to the online setting.},
	language = {en},
	author = {Riquelme, Carlos and Tucker, George and Snoek, Jasper},
	year = {2018},
	pages = {27},
	file = {Riquelme et al. - 2018 - DEEP BAYESIAN BANDITS SHOWDOWN.pdf:/home/ludvig/Zotero/storage/BSYYB96Z/Riquelme et al. - 2018 - DEEP BAYESIAN BANDITS SHOWDOWN.pdf:application/pdf},
}

@article{osband_randomized_2018,
	title = {Randomized {Prior} {Functions} for {Deep} {Reinforcement} {Learning}},
	url = {http://arxiv.org/abs/1806.03335},
	abstract = {Dealing with uncertainty is essential for eﬃcient reinforcement learning. There is a growing literature on uncertainty estimation for deep learning from ﬁxed datasets, but many of the most popular approaches are poorlysuited to sequential decision problems. Other methods, such as bootstrap sampling, have no mechanism for uncertainty that does not come from the observed data. We highlight why this can be a crucial shortcoming and propose a simple remedy through addition of a randomized untrainable ‘prior’ network to each ensemble member. We prove that this approach is eﬃcient with linear representations, provide simple illustrations of its eﬃcacy with nonlinear representations and show that this approach scales to large-scale problems far better than previous attempts.},
	language = {en},
	urldate = {2021-10-28},
	journal = {arXiv:1806.03335 [cs, stat]},
	author = {Osband, Ian and Aslanides, John and Cassirer, Albin},
	month = nov,
	year = {2018},
	note = {arXiv: 1806.03335},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Artificial Intelligence},
	file = {Osband et al. - 2018 - Randomized Prior Functions for Deep Reinforcement .pdf:/home/ludvig/Zotero/storage/QGX8T5FR/Osband et al. - 2018 - Randomized Prior Functions for Deep Reinforcement .pdf:application/pdf},
}

@article{osband_deep_2019,
	title = {Deep {Exploration} via {Randomized} {Value} {Functions}},
	url = {http://arxiv.org/abs/1703.07608},
	abstract = {We study the use of randomized value functions to guide deep exploration in reinforcement learning. This oﬀers an elegant means for synthesizing statistically and computationally eﬃcient exploration with common practical approaches to value function learning. We present several reinforcement learning algorithms that leverage randomized value functions and demonstrate their eﬃcacy through computational studies. We also prove a regret bound that establishes statistical eﬃciency with a tabular representation.},
	language = {en},
	urldate = {2021-10-28},
	journal = {arXiv:1703.07608 [cs, stat]},
	author = {Osband, Ian and Van Roy, Benjamin and Russo, Daniel and Wen, Zheng},
	month = sep,
	year = {2019},
	note = {arXiv: 1703.07608},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Artificial Intelligence},
	file = {Osband et al. - 2019 - Deep Exploration via Randomized Value Functions.pdf:/home/ludvig/Zotero/storage/7UZW8V2P/Osband et al. - 2019 - Deep Exploration via Randomized Value Functions.pdf:application/pdf},
}

@article{ghavamzadeh_bayesian_2015,
	title = {Bayesian {Reinforcement} {Learning}: {A} {Survey}},
	volume = {8},
	issn = {1935-8237, 1935-8245},
	shorttitle = {Bayesian {Reinforcement} {Learning}},
	url = {http://arxiv.org/abs/1609.04436},
	doi = {10.1561/2200000049},
	abstract = {Bayesian methods for machine learning have been widely investigated, yielding principled methods for incorporating prior information into inference algorithms. In this survey, we provide an in-depth review of the role of Bayesian methods for the reinforcement learning (RL) paradigm. The major incentives for incorporating Bayesian reasoning in RL are: 1) it provides an elegant approach to action-selection (exploration/exploitation) as a function of the uncertainty in learning; and 2) it provides a machinery to incorporate prior knowledge into the algorithms. We first discuss models and methods for Bayesian inference in the simple single-step Bandit model. We then review the extensive recent literature on Bayesian methods for model-based RL, where prior information can be expressed on the parameters of the Markov model. We also present Bayesian methods for model-free RL, where priors are expressed over the value function or policy class. The objective of the paper is to provide a comprehensive survey on Bayesian RL algorithms and their theoretical and empirical properties.},
	language = {en},
	number = {5-6},
	urldate = {2021-11-05},
	journal = {Foundations and Trends® in Machine Learning},
	author = {Ghavamzadeh, Mohammad and Mannor, Shie and Pineau, Joelle and Tamar, Aviv},
	year = {2015},
	note = {arXiv: 1609.04436},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Artificial Intelligence},
	pages = {359--483},
	file = {Ghavamzadeh et al. - 2015 - Bayesian Reinforcement Learning A Survey.pdf:/home/ludvig/Zotero/storage/GCU5BZM7/Ghavamzadeh et al. - 2015 - Bayesian Reinforcement Learning A Survey.pdf:application/pdf},
}

@book{wiering_reinforcement_2012,
	address = {Berlin, Heidelberg},
	series = {Adaptation, {Learning}, and {Optimization}},
	title = {Reinforcement {Learning}},
	volume = {12},
	isbn = {978-3-642-27644-6 978-3-642-27645-3},
	url = {http://link.springer.com/10.1007/978-3-642-27645-3},
	language = {en},
	urldate = {2021-11-05},
	publisher = {Springer Berlin Heidelberg},
	editor = {Wiering, Marco and van Otterlo, Martijn},
	year = {2012},
	doi = {10.1007/978-3-642-27645-3},
	file = {Wiering and van Otterlo - 2012 - Reinforcement Learning.pdf:/home/ludvig/Zotero/storage/YBSDDI8M/Wiering and van Otterlo - 2012 - Reinforcement Learning.pdf:application/pdf},
}

@article{shi_spectral_2018,
	title = {A {Spectral} {Approach} to {Gradient} {Estimation} for {Implicit} {Distributions}},
	url = {https://arxiv.org/abs/1806.02925v1},
	abstract = {Recently there have been increasing interests in learning and inference with implicit distributions (i.e., distributions without tractable densities). To this end, we develop a gradient estimator for implicit distributions based on Stein's identity and a spectral decomposition of kernel operators, where the eigenfunctions are approximated by the Nystr{\textbackslash}"om method. Unlike the previous works that only provide estimates at the sample points, our approach directly estimates the gradient function, thus allows for a simple and principled out-of-sample extension. We provide theoretical results on the error bound of the estimator and discuss the bias-variance tradeoff in practice. The effectiveness of our method is demonstrated by applications to gradient-free Hamiltonian Monte Carlo and variational inference with implicit distributions. Finally, we discuss the intuition behind the estimator by drawing connections between the Nystr{\textbackslash}"om method and kernel PCA, which indicates that the estimator can automatically adapt to the geometry of the underlying distribution.},
	language = {en},
	urldate = {2021-11-08},
	author = {Shi, Jiaxin and Sun, Shengyang and Zhu, Jun},
	month = jun,
	year = {2018},
	file = {Snapshot:/home/ludvig/Zotero/storage/L3BKL58T/1806.html:text/html;Full Text PDF:/home/ludvig/Zotero/storage/JY6MCQC6/Shi et al. - 2018 - A Spectral Approach to Gradient Estimation for Imp.pdf:application/pdf},
}

@article{fortunato_noisy_2019,
	title = {Noisy {Networks} for {Exploration}},
	url = {http://arxiv.org/abs/1706.10295},
	abstract = {We introduce NoisyNet, a deep reinforcement learning agent with parametric noise added to its weights, and show that the induced stochasticity of the agent’s policy can be used to aid efﬁcient exploration. The parameters of the noise are learned with gradient descent along with the remaining network weights. NoisyNet is straightforward to implement and adds little computational overhead. We ﬁnd that replacing the conventional exploration heuristics for A3C, DQN and Dueling agents (entropy reward and -greedy respectively) with NoisyNet yields substantially higher scores for a wide range of Atari games, in some cases advancing the agent from sub to super-human performance.},
	language = {en},
	urldate = {2021-12-03},
	journal = {arXiv:1706.10295 [cs, stat]},
	author = {Fortunato, Meire and Azar, Mohammad Gheshlaghi and Piot, Bilal and Menick, Jacob and Osband, Ian and Graves, Alex and Mnih, Vlad and Munos, Remi and Hassabis, Demis and Pietquin, Olivier and Blundell, Charles and Legg, Shane},
	month = jul,
	year = {2019},
	note = {arXiv: 1706.10295},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {Fortunato et al. - 2019 - Noisy Networks for Exploration.pdf:/home/ludvig/Zotero/storage/M6DMZ8JL/Fortunato et al. - 2019 - Noisy Networks for Exploration.pdf:application/pdf},
}
