
@inproceedings{arjovsky_wasserstein_2017,
	address = {International Convention Centre, Sydney, Australia},
	series = {Proceedings of {Machine} {Learning} {Research}},
	title = {Wasserstein {Generative} {Adversarial} {Networks}},
	volume = {70},
	url = {http://proceedings.mlr.press/v70/arjovsky17a.html},
	abstract = {We introduce a new algorithm named WGAN, an alternative to traditional GAN training. In this new model, we show that we can improve the stability of learning, get rid of problems like mode collapse, and provide meaningful learning curves useful for debugging and hyperparameter searches. Furthermore, we show that the corresponding optimization problem is sound, and provide extensive theoretical work highlighting the deep connections to different distances between distributions.},
	booktitle = {Proceedings of the 34th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Arjovsky, Martin and Chintala, Soumith and Bottou, Léon},
	editor = {Precup, Doina and Teh, Yee Whye},
	month = aug,
	year = {2017},
	pages = {214--223},
}

@inproceedings{jiang_approximate_2018,
	address = {Playa Blanca, Lanzarote, Canary Islands},
	series = {Proceedings of {Machine} {Learning} {Research}},
	title = {Approximate {Bayesian} {Computation} with {Kullback}-{Leibler} {Divergence} as {Data} {Discrepancy}},
	volume = {84},
	url = {http://proceedings.mlr.press/v84/jiang18a.html},
	abstract = {Complex simulator-based models usually have intractable likelihood functions, rendering the likelihood-based inference methods inapplicable. Approximate Bayesian Computation (ABC) emerges as an alternative framework of likelihood-free inference methods. It identifies a quasi-posterior distribution by finding values of parameter that simulate the synthetic data resembling the observed data. A major ingredient of ABC is the discrepancy measure between the observed and the simulated data, which conventionally involves a fundamental difficulty of constructing effective summary statistics. To bypass this difficulty, we adopt a Kullback-Leibler divergence estimator to assess the data discrepancy. Our method enjoys the asymptotic consistency and linearithmic time complexity as the data size increases. In experiments on five benchmark models, this method achieves a comparable or higher quasi-posterior quality, compared to the existing methods using other discrepancy measures.},
	booktitle = {Proceedings of the {Twenty}-{First} {International} {Conference} on {Artificial} {Intelligence} and {Statistics}},
	publisher = {PMLR},
	author = {Jiang, Bai},
	editor = {Storkey, Amos and Perez-Cruz, Fernando},
	month = apr,
	year = {2018},
	pages = {1711--1721},
}

@article{goodfellow_generative_2014,
	title = {Generative {Adversarial} {Networks}},
	volume = {3},
	journal = {Advances in Neural Information Processing Systems},
	author = {Goodfellow, Ian and Pouget-Abadie, Jean and Mirza, Mehdi and Xu, Bing and Warde-Farley, David and Ozair, Sherjil and Courville, Aaron and Bengio, Y.},
	year = {2014},
}

@article{pawlowski_implicit_2017,
	title = {Implicit {Weight} {Uncertainty} in {Neural} {Networks}},
	volume = {abs/1711.01297},
	journal = {ArXiv},
	author = {Pawlowski, Nick and Rajchl, Martin and Glocker, Ben},
	year = {2017},
}

@inproceedings{louizos_multiplicative_2017,
	address = {Sydney, NSW, Australia},
	series = {{ICML17}},
	title = {Multiplicative {Normalizing} {Flows} for {Variational} {Bayesian} {Neural} {Networks}},
	booktitle = {Proceedings of the 34th {International} {Conference} on {Machine} {Learning} - {Volume} 70},
	publisher = {JMLR.org},
	author = {Louizos, Christos and Welling, Max},
	year = {2017},
	pages = {2218--2227},
}

@inproceedings{blundell_weight_2015,
	address = {Lille, France},
	series = {Proceedings of {Machine} {Learning} {Research}},
	title = {Weight {Uncertainty} in {Neural} {Network}},
	volume = {37},
	url = {http://proceedings.mlr.press/v37/blundell15.html},
	abstract = {We introduce a new, efficient, principled and backpropagation-compatible algorithm for learning a probability distribution on the weights of a neural network, called Bayes by Backprop. It regularises the weights by minimising a compression cost, known as the variational free energy or the expected lower bound on the marginal likelihood. We show that this principled kind of regularisation yields comparable performance to dropout on MNIST classification. We then demonstrate how the learnt uncertainty in the weights can be used to improve generalisation in non-linear regression problems, and how this weight uncertainty can be used to drive the exploration-exploitation trade-off in reinforcement learning.},
	booktitle = {Proceedings of the 32nd {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Blundell, Charles and Cornebise, Julien and Kavukcuoglu, Koray and Wierstra, Daan},
	editor = {Bach, Francis and Blei, David},
	month = jul,
	year = {2015},
	pages = {1613--1622},
}

@article{choromanska_loss_2015,
	title = {The {Loss} {Surfaces} of {Multilayer} {Networks}},
	url = {http://arxiv.org/abs/1412.0233},
	abstract = {We study the connection between the highly non-convex loss function of a simple model of the fully-connected feed-forward neural network and the Hamiltonian of the spherical spin-glass model under the assumptions of: i) variable independence, ii) redundancy in network parametrization, and iii) uniformity. These assumptions enable us to explain the complexity of the fully decoupled neural network through the prism of the results from random matrix theory. We show that for large-size decoupled networks the lowest critical values of the random loss function form a layered structure and they are located in a well-defined band lower-bounded by the global minimum. The number of local minima outside that band diminishes exponentially with the size of the network. We empirically verify that the mathematical model exhibits similar behavior as the computer simulations, despite the presence of high dependencies in real networks. We conjecture that both simulated annealing and SGD converge to the band of low critical points, and that all critical points found there are local minima of high quality measured by the test error. This emphasizes a major difference between large- and small-size networks where for the latter poor quality local minima have non-zero probability of being recovered. Finally, we prove that recovering the global minimum becomes harder as the network size increases and that it is in practice irrelevant as global minimum often leads to overfitting.},
	urldate = {2020-03-23},
	journal = {arXiv:1412.0233 [cs]},
	author = {Choromanska, Anna and Henaff, Mikael and Mathieu, Michael and Arous, Gérard Ben and LeCun, Yann},
	month = jan,
	year = {2015},
	note = {arXiv: 1412.0233},
	keywords = {Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:/home/ludvig/Zotero/storage/QB24EKTP/Choromanska et al. - 2015 - The Loss Surfaces of Multilayer Networks.pdf:application/pdf},
}

@article{leshno_multilayer_1993,
	title = {Multilayer feedforward networks with a nonpolynomial activation function can approximate any function},
	volume = {6},
	issn = {0893-6080},
	url = {http://www.sciencedirect.com/science/article/pii/S0893608005801315},
	doi = {10.1016/S0893-6080(05)80131-5},
	abstract = {Several researchers characterized the activation function under which multilayer feedforward networks can act as universal approximators. We show that most of all the characterizations that were reported thus far in the literature are special cases of the following general result: A standard multilayer feedforward network with a locally bounded piecewise continuous activation function can approximate any continuous function to any degree of accuracy if and only if the network's activation function is not a polynomial. We also emphasize the important role of the threshold, asserting that without it the last theorem does not hold.},
	language = {en},
	number = {6},
	urldate = {2020-03-16},
	journal = {Neural Networks},
	author = {Leshno, Moshe and Lin, Vladimir Ya. and Pinkus, Allan and Schocken, Shimon},
	month = jan,
	year = {1993},
	keywords = {(μ) approximation, Activation functions, Multilayer feedforward networks, Role of threshold, Universal approximation capabilities},
	pages = {861--867},
	file = {ScienceDirect Full Text PDF:/home/ludvig/Zotero/storage/P2NSAUZZ/Leshno et al. - 1993 - Multilayer feedforward networks with a nonpolynomi.pdf:application/pdf;ScienceDirect Snapshot:/home/ludvig/Zotero/storage/UIUQQD2M/S0893608005801315.html:text/html},
}

@book{goodfellow_deep_2016,
	address = {Cambridge, Massachusetts},
	series = {Adaptive computation and machine learning},
	title = {Deep learning},
	isbn = {978-0-262-03561-3},
	publisher = {The MIT Press},
	author = {Goodfellow, Ian and Bengio, Yoshua and Courville, Aaron},
	year = {2016},
	keywords = {Machine learning},
}

@article{rosenblatt_perceptron_1958,
	title = {The {Perceptron}: {A} {Probabilistic} {Model} for {Information} {Storage} and {Organization} in {The} {Brain}},
	shorttitle = {The {Perceptron}},
	abstract = {If we are eventually to understand the capability of higher organisms for perceptual recognition, generalization, recall, and thinking, we must first have answers to three fundamental questions: 1. How is information about the physical world sensed, or detected, by the biological system? 2. In what form is information stored, or remembered? 3. How does information contained in storage, or in memory, influence recognition and behavior? The first of these questions is in the},
	journal = {Psychological Review},
	author = {Rosenblatt, F.},
	year = {1958},
	pages = {65--386},
	file = {Citeseer - Full Text PDF:/home/ludvig/Zotero/storage/MRJQQJWS/Rosenblatt - 1958 - The Perceptron A Probabilistic Model for Informat.pdf:application/pdf;Citeseer - Snapshot:/home/ludvig/Zotero/storage/PIQEZA6G/summary.html:text/html},
}

@article{gammels_committee_nodate,
	title = {A {Committee} of {One}},
	language = {en},
	author = {Gammels, Martin},
	pages = {80},
	file = {Gammels - A Committee of One.pdf:/home/ludvig/Zotero/storage/V3S6L352/Gammels - A Committee of One.pdf:application/pdf},
}

@article{kullback_information_1951,
	title = {On {Information} and {Sufficiency}},
	volume = {22},
	issn = {0003-4851},
	url = {https://www.jstor.org/stable/2236703},
	number = {1},
	urldate = {2020-03-02},
	journal = {The Annals of Mathematical Statistics},
	author = {Kullback, S. and Leibler, R. A.},
	year = {1951},
	note = {Publisher: Institute of Mathematical Statistics},
	pages = {79--86},
	file = {JSTOR Full Text PDF:/home/ludvig/Zotero/storage/PB9BT8R3/Kullback and Leibler - 1951 - On Information and Sufficiency.pdf:application/pdf},
}

@article{roberts_weak_1997,
	title = {Weak {Convergence} and {Optimal} {Scaling} of {Random} {Walk} {Metropolis} {Algorithms}},
	volume = {7},
	issn = {1050-5164},
	url = {https://www.jstor.org/stable/2245134},
	abstract = {This paper considers the problem of scaling the proposal distribution of a multidimensional random walk Metropolis algorithm in order to maximize the efficiency of the algorithm. The main result is a weak convergence result as the dimension of a sequence of target densities, n, converges to ∞. When the proposal variance is appropriately scaled according to n, the sequence of stochastic processes formed by the first component of each Markov chain converges to the appropriate limiting Langevin diffusion process. The limiting diffusion approximation admits a straightforward efficiency maximization problem, and the resulting asymptotically optimal policy is related to the asymptotic acceptance rate of proposed moves for the algorithm. The asymptotically optimal acceptance rate is 0.234 under quite general conditions. The main result is proved in the case where the target density has a symmetric product form. Extensions of the result are discussed.},
	number = {1},
	urldate = {2020-03-02},
	journal = {The Annals of Applied Probability},
	author = {Roberts, G. O. and Gelman, A. and Gilks, W. R.},
	year = {1997},
	note = {Publisher: Institute of Mathematical Statistics},
	pages = {110--120},
	file = {JSTOR Full Text PDF:/home/ludvig/Zotero/storage/7C9G5W4Z/Roberts et al. - 1997 - Weak Convergence and Optimal Scaling of Random Wal.pdf:application/pdf},
}

@article{lecun_mnist_2010,
	title = {{MNIST} handwritten digit database},
	url = {http://yann.lecun.com/exdb/mnist/},
	urldate = {2016-01-14},
	author = {LeCun, Yann and Cortes, Corinna},
	year = {2010},
	keywords = {MSc \_checked character\_recognition mnist network neural},
}

@misc{clanuwat_deep_2018,
	title = {Deep {Learning} for {Classical} {Japanese} {Literature}},
	author = {Clanuwat, Tarin and Bober-Irizar, Mikel and Kitamoto, Asanobu and Lamb, Alex and Yamamoto, Kazuaki and Ha, David},
	month = dec,
	year = {2018},
	note = {arXiv: cs.CV/1812.01718},
}

@article{robbins_stochastic_1951,
	title = {A {Stochastic} {Approximation} {Method}},
	volume = {22},
	issn = {0003-4851, 2168-8990},
	url = {https://projecteuclid.org/euclid.aoms/1177729586},
	doi = {10.1214/aoms/1177729586},
	abstract = {Let M(x)M(x)M(x) denote the expected value at level xxx of the response to a certain experiment. M(x)M(x)M(x) is assumed to be a monotone function of xxx but is unknown to the experimenter, and it is desired to find the solution x=θx=θx = {\textbackslash}theta of the equation M(x)=αM(x)=αM(x) = {\textbackslash}alpha, where αα{\textbackslash}alpha is a given constant. We give a method for making successive experiments at levels x1,x2,⋯x1,x2,⋯x\_1,x\_2,{\textbackslash}cdots in such a way that xnxnx\_n will tend to θθ{\textbackslash}theta in probability.},
	language = {EN},
	number = {3},
	urldate = {2020-10-05},
	journal = {Annals of Mathematical Statistics},
	author = {Robbins, Herbert and Monro, Sutton},
	month = sep,
	year = {1951},
	mrnumber = {MR42668},
	zmnumber = {0054.05901},
	note = {Publisher: Institute of Mathematical Statistics},
	pages = {400--407},
	file = {Full Text PDF:/home/ludvig/Zotero/storage/78C29WRH/Robbins and Monro - 1951 - A Stochastic Approximation Method.pdf:application/pdf;Snapshot:/home/ludvig/Zotero/storage/6HLYTVRF/1177729586.html:text/html},
}

@incollection{ghahramani_propagation_2001,
	title = {Propagation {Algorithms} for {Variational} {Bayesian} {Learning}},
	url = {http://papers.nips.cc/paper/1907-propagation-algorithms-for-variational-bayesian-learning.pdf},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems} 13},
	publisher = {MIT Press},
	author = {Ghahramani, Zoubin and Beal, Matthew J.},
	editor = {Leen, T. K. and Dietterich, T. G. and Tresp, V.},
	year = {2001},
	pages = {507--513},
}

@article{ranganath_black_2013,
	title = {Black {Box} {Variational} {Inference}},
	url = {http://arxiv.org/abs/1401.0118},
	abstract = {Variational inference has become a widely used method to approximate posteriors in complex latent variables models. However, deriving a variational inference algorithm generally requires significant model-specific analysis, and these efforts can hinder and deter us from quickly developing and exploring a variety of models for a problem at hand. In this paper, we present a "black box" variational inference algorithm, one that can be quickly applied to many models with little additional derivation. Our method is based on a stochastic optimization of the variational objective where the noisy gradient is computed from Monte Carlo samples from the variational distribution. We develop a number of methods to reduce the variance of the gradient, always maintaining the criterion that we want to avoid difficult model-based derivations. We evaluate our method against the corresponding black box sampling based methods. We find that our method reaches better predictive likelihoods much faster than sampling methods. Finally, we demonstrate that Black Box Variational Inference lets us easily explore a wide space of models by quickly constructing and evaluating several models of longitudinal healthcare data.},
	urldate = {2020-10-05},
	journal = {arXiv:1401.0118 [cs, stat]},
	author = {Ranganath, Rajesh and Gerrish, Sean and Blei, David M.},
	month = dec,
	year = {2013},
	note = {arXiv: 1401.0118},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Statistics - Computation, Statistics - Methodology},
	file = {arXiv.org Snapshot:/home/ludvig/Zotero/storage/JDKAQUPZ/1401.html:text/html;arXiv Fulltext PDF:/home/ludvig/Zotero/storage/GJ35XRLK/Ranganath et al. - 2013 - Black Box Variational Inference.pdf:application/pdf},
}

@article{ghahramani_propagation_nodate,
	title = {Propagation {Algorithms} for {Variational} {Bayesian} {Learning}},
	abstract = {Variational approximations are becoming a widespread tool for Bayesian learning of graphical models. We provide some theoretical results for the variational updates in a very general family of conjugate-exponential graphical models. We show how the belief propagation and the junction tree algorithms can be used in the inference step of variational Bayesian learning. Applying these results to the Bayesian analysis of linear-Gaussian state-space models we obtain a learning procedure that exploits the Kalman smoothing propagation, while integrating over all model parameters. We demonstrate how this can be used to infer the hidden state dimensionality of the state-space model in a variety of synthetic problems and one real high-dimensional data set.},
	language = {en},
	author = {Ghahramani, Zoubin and Beal, Matthew J},
	pages = {7},
	file = {Ghahramani and Beal - Propagation Algorithms for Variational Bayesian Le.pdf:/home/ludvig/Zotero/storage/6JFR2DSJ/Ghahramani and Beal - Propagation Algorithms for Variational Bayesian Le.pdf:application/pdf},
}

@article{rezende_variational_2016,
	title = {Variational {Inference} with {Normalizing} {Flows}},
	url = {http://arxiv.org/abs/1505.05770},
	abstract = {The choice of approximate posterior distribution is one of the core problems in variational inference. Most applications of variational inference employ simple families of posterior approximations in order to allow for efﬁcient inference, focusing on mean-ﬁeld or other simple structured approximations. This restriction has a signiﬁcant impact on the quality of inferences made using variational methods. We introduce a new approach for specifying ﬂexible, arbitrarily complex and scalable approximate posterior distributions. Our approximations are distributions constructed through a normalizing ﬂow, whereby a simple initial density is transformed into a more complex one by applying a sequence of invertible transformations until a desired level of complexity is attained. We use this view of normalizing ﬂows to develop categories of ﬁnite and inﬁnitesimal ﬂows and provide a uniﬁed view of approaches for constructing rich posterior approximations. We demonstrate that the theoretical advantages of having posteriors that better match the true posterior, combined with the scalability of amortized variational approaches, provides a clear improvement in performance and applicability of variational inference.},
	language = {en},
	urldate = {2020-10-11},
	journal = {arXiv:1505.05770 [cs, stat]},
	author = {Rezende, Danilo Jimenez and Mohamed, Shakir},
	month = jun,
	year = {2016},
	note = {arXiv: 1505.05770},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Statistics - Computation, Statistics - Methodology, Computer Science - Artificial Intelligence},
	file = {Rezende and Mohamed - 2016 - Variational Inference with Normalizing Flows.pdf:/home/ludvig/Zotero/storage/JFDTJF8Q/Rezende and Mohamed - 2016 - Variational Inference with Normalizing Flows.pdf:application/pdf},
}

@article{dinh_nice_2015,
	title = {{NICE}: {Non}-linear {Independent} {Components} {Estimation}},
	shorttitle = {{NICE}},
	url = {http://arxiv.org/abs/1410.8516},
	abstract = {We propose a deep learning framework for modeling complex high-dimensional densities called Non-linear Independent Component Estimation (NICE). It is based on the idea that a good representation is one in which the data has a distribution that is easy to model. For this purpose, a non-linear deterministic transformation of the data is learned that maps it to a latent space so as to make the transformed data conform to a factorized distribution, i.e., resulting in independent latent variables. We parametrize this transformation so that computing the determinant of the Jacobian and inverse Jacobian is trivial, yet we maintain the ability to learn complex non-linear transformations, via a composition of simple building blocks, each based on a deep neural network. The training criterion is simply the exact log-likelihood, which is tractable. Unbiased ancestral sampling is also easy. We show that this approach yields good generative models on four image datasets and can be used for inpainting.},
	language = {en},
	urldate = {2020-10-12},
	journal = {arXiv:1410.8516 [cs]},
	author = {Dinh, Laurent and Krueger, David and Bengio, Yoshua},
	month = apr,
	year = {2015},
	note = {arXiv: 1410.8516},
	keywords = {Computer Science - Machine Learning},
	file = {Dinh et al. - 2015 - NICE Non-linear Independent Components Estimation.pdf:/home/ludvig/Zotero/storage/SWZKBV8R/Dinh et al. - 2015 - NICE Non-linear Independent Components Estimation.pdf:application/pdf},
}

@article{dinh_density_2017,
	title = {Density estimation using {Real} {NVP}},
	url = {http://arxiv.org/abs/1605.08803},
	abstract = {Unsupervised learning of probabilistic models is a central yet challenging problem in machine learning. Specifically, designing models with tractable learning, sampling, inference and evaluation is crucial in solving this task. We extend the space of such models using real-valued non-volume preserving (real NVP) transformations, a set of powerful invertible and learnable transformations, resulting in an unsupervised learning algorithm with exact log-likelihood computation, exact sampling, exact inference of latent variables, and an interpretable latent space. We demonstrate its ability to model natural images on four datasets through sampling, log-likelihood evaluation and latent variable manipulations.},
	urldate = {2020-10-12},
	journal = {arXiv:1605.08803 [cs, stat]},
	author = {Dinh, Laurent and Sohl-Dickstein, Jascha and Bengio, Samy},
	month = feb,
	year = {2017},
	note = {arXiv: 1605.08803},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Artificial Intelligence, Computer Science - Neural and Evolutionary Computing},
	file = {arXiv.org Snapshot:/home/ludvig/Zotero/storage/AMPWYIC2/1605.html:text/html;arXiv Fulltext PDF:/home/ludvig/Zotero/storage/2U5W3QH7/Dinh et al. - 2017 - Density estimation using Real NVP.pdf:application/pdf},
}

@article{lecun_mnist_2010-1,
	title = {{MNIST} handwritten digit database},
	volume = {2},
	journal = {ATT Labs [Online]. Available: http://yann.lecun.com/exdb/mnist},
	author = {LeCun, Yann and Cortes, Corinna and Burges, CJ},
	year = {2010},
}

@misc{bulatov_yaroslab_notmnist_2011,
	type = {Blogpost},
	title = {{notMNIST}},
	url = {http://yaroslavvb.blogspot.com/2011/09/notmnist-dataset.html},
	journal = {notMNIST dataset},
	author = {Bulatov, Yaroslab},
	month = sep,
	year = {2011},
}

@article{radford_unsupervised_2016,
	title = {Unsupervised {Representation} {Learning} with {Deep} {Convolutional} {Generative} {Adversarial} {Networks}},
	url = {http://arxiv.org/abs/1511.06434},
	abstract = {In recent years, supervised learning with convolutional networks (CNNs) has seen huge adoption in computer vision applications. Comparatively, unsupervised learning with CNNs has received less attention. In this work we hope to help bridge the gap between the success of CNNs for supervised learning and unsupervised learning. We introduce a class of CNNs called deep convolutional generative adversarial networks (DCGANs), that have certain architectural constraints, and demonstrate that they are a strong candidate for unsupervised learning. Training on various image datasets, we show convincing evidence that our deep convolutional adversarial pair learns a hierarchy of representations from object parts to scenes in both the generator and discriminator. Additionally, we use the learned features for novel tasks - demonstrating their applicability as general image representations.},
	urldate = {2020-11-05},
	journal = {arXiv:1511.06434 [cs]},
	author = {Radford, Alec and Metz, Luke and Chintala, Soumith},
	month = jan,
	year = {2016},
	note = {arXiv: 1511.06434},
	keywords = {Computer Science - Machine Learning, Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:/home/ludvig/Zotero/storage/DCFALTNU/Radford et al. - 2016 - Unsupervised Representation Learning with Deep Con.pdf:application/pdf;arXiv.org Snapshot:/home/ludvig/Zotero/storage/9QFVFRRB/1511.html:text/html},
}

@article{radford_unsupervised_2016-1,
	title = {Unsupervised {Representation} {Learning} with {Deep} {Convolutional} {Generative} {Adversarial} {Networks}},
	url = {http://arxiv.org/abs/1511.06434},
	abstract = {In recent years, supervised learning with convolutional networks (CNNs) has seen huge adoption in computer vision applications. Comparatively, unsupervised learning with CNNs has received less attention. In this work we hope to help bridge the gap between the success of CNNs for supervised learning and unsupervised learning. We introduce a class of CNNs called deep convolutional generative adversarial networks (DCGANs), that have certain architectural constraints, and demonstrate that they are a strong candidate for unsupervised learning. Training on various image datasets, we show convincing evidence that our deep convolutional adversarial pair learns a hierarchy of representations from object parts to scenes in both the generator and discriminator. Additionally, we use the learned features for novel tasks - demonstrating their applicability as general image representations.},
	language = {en},
	urldate = {2020-11-05},
	journal = {arXiv:1511.06434 [cs]},
	author = {Radford, Alec and Metz, Luke and Chintala, Soumith},
	month = jan,
	year = {2016},
	note = {arXiv: 1511.06434},
	keywords = {Computer Science - Machine Learning, Computer Science - Computer Vision and Pattern Recognition},
	file = {Radford et al. - 2016 - Unsupervised Representation Learning with Deep Con.pdf:/home/ludvig/Zotero/storage/7LGCFQU2/Radford et al. - 2016 - Unsupervised Representation Learning with Deep Con.pdf:application/pdf},
}

@article{blei_variational_2017,
	title = {Variational {Inference}: {A} {Review} for {Statisticians}},
	volume = {112},
	issn = {0162-1459, 1537-274X},
	shorttitle = {Variational {Inference}},
	url = {http://arxiv.org/abs/1601.00670},
	doi = {10.1080/01621459.2017.1285773},
	abstract = {One of the core problems of modern statistics is to approximate difficult-to-compute probability densities. This problem is especially important in Bayesian statistics, which frames all inference about unknown quantities as a calculation involving the posterior density. In this paper, we review variational inference (VI), a method from machine learning that approximates probability densities through optimization. VI has been used in many applications and tends to be faster than classical methods, such as Markov chain Monte Carlo sampling. The idea behind VI is to first posit a family of densities and then to find the member of that family which is close to the target. Closeness is measured by Kullback-Leibler divergence. We review the ideas behind mean-field variational inference, discuss the special case of VI applied to exponential family models, present a full example with a Bayesian mixture of Gaussians, and derive a variant that uses stochastic optimization to scale up to massive data. We discuss modern research in VI and highlight important open problems. VI is powerful, but it is not yet well understood. Our hope in writing this paper is to catalyze statistical research on this class of algorithms.},
	number = {518},
	urldate = {2020-11-30},
	journal = {Journal of the American Statistical Association},
	author = {Blei, David M. and Kucukelbir, Alp and McAuliffe, Jon D.},
	month = apr,
	year = {2017},
	note = {arXiv: 1601.00670},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Statistics - Computation},
	pages = {859--877},
	file = {arXiv Fulltext PDF:/home/ludvig/Zotero/storage/KLMKX3SR/Blei et al. - 2017 - Variational Inference A Review for Statisticians.pdf:application/pdf;arXiv.org Snapshot:/home/ludvig/Zotero/storage/TXUHQT9X/1601.html:text/html},
}

@article{foong_expressiveness_2020,
	title = {On the {Expressiveness} of {Approximate} {Inference} in {Bayesian} {Neural} {Networks}},
	url = {http://arxiv.org/abs/1909.00719},
	abstract = {While Bayesian neural networks (BNNs) hold the promise of being flexible, well-calibrated statistical models, inference often requires approximations whose consequences are poorly understood. We study the quality of common variational methods in approximating the Bayesian predictive distribution. For single-hidden layer ReLU BNNs, we prove a fundamental limitation in function-space of two of the most commonly used distributions defined in weight-space: mean-field Gaussian and Monte Carlo dropout. We find there are simple cases where neither method can have substantially increased uncertainty in between well-separated regions of low uncertainty. We provide strong empirical evidence that exact inference does not have this pathology, hence it is due to the approximation and not the model. In contrast, for deep networks, we prove a universality result showing that there exist approximate posteriors in the above classes which provide flexible uncertainty estimates. However, we find empirically that pathologies of a similar form as in the single-hidden layer case can persist when performing variational inference in deeper networks. Our results motivate careful consideration of the implications of approximate inference methods in BNNs.},
	urldate = {2020-12-14},
	journal = {arXiv:1909.00719 [cs, stat]},
	author = {Foong, Andrew Y. K. and Burt, David R. and Li, Yingzhen and Turner, Richard E.},
	month = oct,
	year = {2020},
	note = {arXiv: 1909.00719},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/home/ludvig/Zotero/storage/DBT66ZL3/Foong et al. - 2020 - On the Expressiveness of Approximate Inference in .pdf:application/pdf;arXiv.org Snapshot:/home/ludvig/Zotero/storage/NI36UWFR/1909.html:text/html},
}

@article{foong_expressiveness_nodate,
	title = {On the {Expressiveness} of {Approximate} {Inference} in {Bayesian} {Neural} {Networks}},
	abstract = {While Bayesian neural networks (BNNs) hold the promise of being ﬂexible, wellcalibrated statistical models, inference often requires approximations whose consequences are poorly understood. We study the quality of common variational methods in approximating the Bayesian predictive distribution. For single-hidden layer ReLU BNNs, we prove a fundamental limitation in function-space of two of the most commonly used distributions deﬁned in weight-space: mean-ﬁeld Gaussian and Monte Carlo dropout. We ﬁnd there are simple cases where neither method can have substantially increased uncertainty in between well-separated regions of low uncertainty. We provide strong empirical evidence that exact inference does not have this pathology, hence it is due to the approximation and not the model. In contrast, for deep networks, we prove a universality result showing that there exist approximate posteriors in the above classes which provide ﬂexible uncertainty estimates. However, we ﬁnd empirically that pathologies of a similar form as in the single-hidden layer case can persist when performing variational inference in deeper networks. Our results motivate careful consideration of the implications of approximate inference methods in BNNs.},
	language = {en},
	author = {Foong, Andrew Y K and Burt, David R and Li, Yingzhen and Turner, Richard E},
	pages = {12},
	file = {Foong et al. - On the Expressiveness of Approximate Inference in .pdf:/home/ludvig/Zotero/storage/S6S9DINS/Foong et al. - On the Expressiveness of Approximate Inference in .pdf:application/pdf},
}

@article{foong_expressiveness_nodate-1,
	title = {On the {Expressiveness} of {Approximate} {Inference} in {Bayesian} {Neural} {Networks}},
	abstract = {While Bayesian neural networks (BNNs) hold the promise of being ﬂexible, wellcalibrated statistical models, inference often requires approximations whose consequences are poorly understood. We study the quality of common variational methods in approximating the Bayesian predictive distribution. For single-hidden layer ReLU BNNs, we prove a fundamental limitation in function-space of two of the most commonly used distributions deﬁned in weight-space: mean-ﬁeld Gaussian and Monte Carlo dropout. We ﬁnd there are simple cases where neither method can have substantially increased uncertainty in between well-separated regions of low uncertainty. We provide strong empirical evidence that exact inference does not have this pathology, hence it is due to the approximation and not the model. In contrast, for deep networks, we prove a universality result showing that there exist approximate posteriors in the above classes which provide ﬂexible uncertainty estimates. However, we ﬁnd empirically that pathologies of a similar form as in the single-hidden layer case can persist when performing variational inference in deeper networks. Our results motivate careful consideration of the implications of approximate inference methods in BNNs.},
	language = {en},
	author = {Foong, Andrew Y K and Burt, David R and Li, Yingzhen and Turner, Richard E},
	pages = {12},
	file = {Foong et al. - On the Expressiveness of Approximate Inference in .pdf:/home/ludvig/Zotero/storage/UTYI2AN7/Foong et al. - On the Expressiveness of Approximate Inference in .pdf:application/pdf},
}

@article{foong_expressiveness_nodate-2,
	title = {On the {Expressiveness} of {Approximate} {Inference} in {Bayesian} {Neural} {Networks}},
	abstract = {While Bayesian neural networks (BNNs) hold the promise of being ﬂexible, wellcalibrated statistical models, inference often requires approximations whose consequences are poorly understood. We study the quality of common variational methods in approximating the Bayesian predictive distribution. For single-hidden layer ReLU BNNs, we prove a fundamental limitation in function-space of two of the most commonly used distributions deﬁned in weight-space: mean-ﬁeld Gaussian and Monte Carlo dropout. We ﬁnd there are simple cases where neither method can have substantially increased uncertainty in between well-separated regions of low uncertainty. We provide strong empirical evidence that exact inference does not have this pathology, hence it is due to the approximation and not the model. In contrast, for deep networks, we prove a universality result showing that there exist approximate posteriors in the above classes which provide ﬂexible uncertainty estimates. However, we ﬁnd empirically that pathologies of a similar form as in the single-hidden layer case can persist when performing variational inference in deeper networks. Our results motivate careful consideration of the implications of approximate inference methods in BNNs.},
	language = {en},
	author = {Foong, Andrew Y K and Burt, David R and Li, Yingzhen and Turner, Richard E},
	pages = {12},
	file = {Foong et al. - On the Expressiveness of Approximate Inference in .pdf:/home/ludvig/Zotero/storage/M8VRAUC8/Foong et al. - On the Expressiveness of Approximate Inference in .pdf:application/pdf},
}

@article{yao_quality_2019,
	title = {Quality of {Uncertainty} {Quantification} for {Bayesian} {Neural} {Network} {Inference}},
	url = {http://arxiv.org/abs/1906.09686},
	abstract = {Bayesian Neural Networks (BNNs) place priors over the parameters in a neural network. Inference in BNNs, however, is difficult; all inference methods for BNNs are approximate. In this work, we empirically compare the quality of predictive uncertainty estimates for 10 common inference methods on both regression and classification tasks. Our experiments demonstrate that commonly used metrics (e.g. test log-likelihood) can be misleading. Our experiments also indicate that inference innovations designed to capture structure in the posterior do not necessarily produce high quality posterior approximations.},
	urldate = {2020-12-21},
	journal = {arXiv:1906.09686 [cs, stat]},
	author = {Yao, Jiayu and Pan, Weiwei and Ghosh, Soumya and Doshi-Velez, Finale},
	month = jun,
	year = {2019},
	note = {arXiv: 1906.09686},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/home/ludvig/Zotero/storage/793Q2T6Q/Yao et al. - 2019 - Quality of Uncertainty Quantification for Bayesian.pdf:application/pdf;arXiv.org Snapshot:/home/ludvig/Zotero/storage/YYP29NKA/1906.html:text/html},
}

@article{steinbrener_measuring_2020,
	title = {Measuring the {Uncertainty} of {Predictions} in {Deep} {Neural} {Networks} with {Variational} {Inference}},
	volume = {20},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	url = {https://www.mdpi.com/1424-8220/20/21/6011},
	doi = {10.3390/s20216011},
	abstract = {We present a novel approach for training deep neural networks in a Bayesian way. Compared to other Bayesian deep learning formulations, our approach allows for quantifying the uncertainty in model parameters while only adding very few additional parameters to be optimized. The proposed approach uses variational inference to approximate the intractable a posteriori distribution on basis of a normal prior. By representing the a posteriori uncertainty of the network parameters per network layer and depending on the estimated parameter expectation values, only very few additional parameters need to be optimized compared to a non-Bayesian network. We compare our approach to classical deep learning, Bernoulli dropout and Bayes by Backprop using the MNIST dataset. Compared to classical deep learning, the test error is reduced by 15\%. We also show that the uncertainty information obtained can be used to calculate credible intervals for the network prediction and to optimize network architecture for the dataset at hand. To illustrate that our approach also scales to large networks and input vector sizes, we apply it to the GoogLeNet architecture on a custom dataset, achieving an average accuracy of 0.92. Using 95\% credible intervals, all but one wrong classification result can be detected.},
	language = {en},
	number = {21},
	urldate = {2020-12-21},
	journal = {Sensors},
	author = {Steinbrener, Jan and Posch, Konstantin and Pilz, Jürgen},
	month = jan,
	year = {2020},
	note = {Number: 21
Publisher: Multidisciplinary Digital Publishing Institute},
	keywords = {Bayesian deep learning, image classification, model uncertainty, variational inference},
	pages = {6011},
	file = {Full Text PDF:/home/ludvig/Zotero/storage/ATMJ4Y2B/Steinbrener et al. - 2020 - Measuring the Uncertainty of Predictions in Deep N.pdf:application/pdf;Snapshot:/home/ludvig/Zotero/storage/EPSVGEKR/6011.html:text/html},
}

@article{steinbrener_measuring_2020-1,
	title = {Measuring the {Uncertainty} of {Predictions} in {Deep} {Neural} {Networks} with {Variational} {Inference}},
	volume = {20},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	url = {https://www.mdpi.com/1424-8220/20/21/6011},
	doi = {10.3390/s20216011},
	abstract = {We present a novel approach for training deep neural networks in a Bayesian way. Compared to other Bayesian deep learning formulations, our approach allows for quantifying the uncertainty in model parameters while only adding very few additional parameters to be optimized. The proposed approach uses variational inference to approximate the intractable a posteriori distribution on basis of a normal prior. By representing the a posteriori uncertainty of the network parameters per network layer and depending on the estimated parameter expectation values, only very few additional parameters need to be optimized compared to a non-Bayesian network. We compare our approach to classical deep learning, Bernoulli dropout and Bayes by Backprop using the MNIST dataset. Compared to classical deep learning, the test error is reduced by 15\%. We also show that the uncertainty information obtained can be used to calculate credible intervals for the network prediction and to optimize network architecture for the dataset at hand. To illustrate that our approach also scales to large networks and input vector sizes, we apply it to the GoogLeNet architecture on a custom dataset, achieving an average accuracy of 0.92. Using 95\% credible intervals, all but one wrong classification result can be detected.},
	language = {en},
	number = {21},
	urldate = {2020-12-21},
	journal = {Sensors},
	author = {Steinbrener, Jan and Posch, Konstantin and Pilz, Jürgen},
	month = jan,
	year = {2020},
	note = {Number: 21
Publisher: Multidisciplinary Digital Publishing Institute},
	keywords = {Bayesian deep learning, image classification, model uncertainty, variational inference},
	pages = {6011},
	file = {Full Text PDF:/home/ludvig/Zotero/storage/AFJMGYVE/Steinbrener et al. - 2020 - Measuring the Uncertainty of Predictions in Deep N.pdf:application/pdf;Snapshot:/home/ludvig/Zotero/storage/4QX4LQYV/6011.html:text/html},
}

@article{hernandez-lobato_probabilistic_nodate,
	title = {Probabilistic {Backpropagation} for {Scalable} {Learning} of {Bayesian} {Neural} {Networks}},
	abstract = {Large multilayer neural networks trained with backpropagation have recently achieved state-ofthe-art results in a wide range of problems. However, using backprop for neural net learning still has some disadvantages, e.g., having to tune a large number of hyperparameters to the data, lack of calibrated probabilistic predictions, and a tendency to overﬁt the training data. In principle, the Bayesian approach to learning neural networks does not have these problems. However, existing Bayesian techniques lack scalability to large dataset and network sizes. In this work we present a novel scalable method for learning Bayesian neural networks, called probabilistic backpropagation (PBP). Similar to classical backpropagation, PBP works by computing a forward propagation of probabilities through the network and then doing a backward computation of gradients. A series of experiments on ten real-world datasets show that PBP is signiﬁcantly faster than other techniques, while offering competitive predictive abilities. Our experiments also show that PBP provides accurate estimates of the posterior variance on the network weights.},
	language = {en},
	author = {Hernández-Lobato, José Miguel and Adams, Ryan P},
	pages = {9},
	file = {Hernández-Lobato and Adams - Probabilistic Backpropagation for Scalable Learnin.pdf:/home/ludvig/Zotero/storage/IVARBACZ/Hernández-Lobato and Adams - Probabilistic Backpropagation for Scalable Learnin.pdf:application/pdf},
}

@article{foong_expressiveness_nodate-3,
	title = {On the {Expressiveness} of {Approximate} {Inference} in {Bayesian} {Neural} {Networks}},
	abstract = {While Bayesian neural networks (BNNs) hold the promise of being ﬂexible, wellcalibrated statistical models, inference often requires approximations whose consequences are poorly understood. We study the quality of common variational methods in approximating the Bayesian predictive distribution. For single-hidden layer ReLU BNNs, we prove a fundamental limitation in function-space of two of the most commonly used distributions deﬁned in weight-space: mean-ﬁeld Gaussian and Monte Carlo dropout. We ﬁnd there are simple cases where neither method can have substantially increased uncertainty in between well-separated regions of low uncertainty. We provide strong empirical evidence that exact inference does not have this pathology, hence it is due to the approximation and not the model. In contrast, for deep networks, we prove a universality result showing that there exist approximate posteriors in the above classes which provide ﬂexible uncertainty estimates. However, we ﬁnd empirically that pathologies of a similar form as in the single-hidden layer case can persist when performing variational inference in deeper networks. Our results motivate careful consideration of the implications of approximate inference methods in BNNs.},
	language = {en},
	author = {Foong, Andrew Y K and Burt, David R and Li, Yingzhen and Turner, Richard E},
	pages = {12},
	file = {Foong et al. - On the Expressiveness of Approximate Inference in .pdf:/home/ludvig/Zotero/storage/J3XW268X/Foong et al. - On the Expressiveness of Approximate Inference in .pdf:application/pdf},
}

@article{arjovsky_towards_2017,
	title = {Towards {Principled} {Methods} for {Training} {Generative} {Adversarial} {Networks}},
	abstract = {The goal of this paper is not to introduce a single algorithm or method, but to make theoretical steps towards fully understanding the training dynamics of generative adversarial networks. In order to substantiate our theoretical analysis, we perform targeted experiments to verify our assumptions, illustrate our claims, and quantify the phenomena. This paper is divided into three sections. The first section introduces the problem at hand. The second section is dedicated to studying and proving rigorously the problems including instability and saturation that arize when training generative adversarial networks. The third section examines a practical and theoretically grounded direction towards solving these problems, while introducing new tools to study them.},
	journal = {ICLR},
	author = {Arjovsky, Martín and Bottou, L.},
	year = {2017},
	file = {Full Text PDF:/home/ludvig/Zotero/storage/294T6ABU/Arjovsky and Bottou - 2017 - Towards Principled Methods for Training Generative.pdf:application/pdf},
}

@article{arjovsky_towards_2017-1,
	title = {{TOWARDS} {PRINCIPLED} {METHODS} {FOR} {TRAINING} {GENERATIVE} {ADVERSARIAL} {NETWORKS}},
	abstract = {The goal of this paper is not to introduce a single algorithm or method, but to make theoretical steps towards fully understanding the training dynamics of generative adversarial networks. In order to substantiate our theoretical analysis, we perform targeted experiments to verify our assumptions, illustrate our claims, and quantify the phenomena. This paper is divided into three sections. The ﬁrst section introduces the problem at hand. The second section is dedicated to studying and proving rigorously the problems including instability and saturation that arize when training generative adversarial networks. The third section examines a practical and theoretically grounded direction towards solving these problems, while introducing new tools to study them.},
	language = {en},
	author = {Arjovsky, Martin and Bottou, Leon},
	year = {2017},
	pages = {17},
	file = {Arjovsky and Bottou - 2017 - TOWARDS PRINCIPLED METHODS FOR TRAINING GENERATIVE.pdf:/home/ludvig/Zotero/storage/CHYXYTPF/Arjovsky and Bottou - 2017 - TOWARDS PRINCIPLED METHODS FOR TRAINING GENERATIVE.pdf:application/pdf},
}

@article{glorot_understanding_nodate,
	title = {Understanding the difﬁculty of training deep feedforward neural networks},
	abstract = {Whereas before 2006 it appears that deep multilayer neural networks were not successfully trained, since then several algorithms have been shown to successfully train them, with experimental results showing the superiority of deeper vs less deep architectures. All these experimental results were obtained with new initialization or training mechanisms. Our objective here is to understand better why standard gradient descent from random initialization is doing so poorly with deep neural networks, to better understand these recent relative successes and help design better algorithms in the future. We ﬁrst observe the inﬂuence of the non-linear activations functions. We ﬁnd that the logistic sigmoid activation is unsuited for deep networks with random initialization because of its mean value, which can drive especially the top hidden layer into saturation. Surprisingly, we ﬁnd that saturated units can move out of saturation by themselves, albeit slowly, and explaining the plateaus sometimes seen when training neural networks. We ﬁnd that a new non-linearity that saturates less can often be beneﬁcial. Finally, we study how activations and gradients vary across layers and during training, with the idea that training may be more difﬁcult when the singular values of the Jacobian associated with each layer are far from 1. Based on these considerations, we propose a new initialization scheme that brings substantially faster convergence.},
	language = {en},
	author = {Glorot, Xavier and Bengio, Yoshua},
	pages = {8},
	file = {Glorot and Bengio - Understanding the difﬁculty of training deep feedf.pdf:/home/ludvig/Zotero/storage/N4DSWGKK/Glorot and Bengio - Understanding the difﬁculty of training deep feedf.pdf:application/pdf},
}

@article{blundell_weight_2015-1,
	title = {Weight {Uncertainty} in {Neural} {Networks}},
	url = {http://arxiv.org/abs/1505.05424},
	abstract = {We introduce a new, efficient, principled and backpropagation-compatible algorithm for learning a probability distribution on the weights of a neural network, called Bayes by Backprop. It regularises the weights by minimising a compression cost, known as the variational free energy or the expected lower bound on the marginal likelihood. We show that this principled kind of regularisation yields comparable performance to dropout on MNIST classification. We then demonstrate how the learnt uncertainty in the weights can be used to improve generalisation in non-linear regression problems, and how this weight uncertainty can be used to drive the exploration-exploitation trade-off in reinforcement learning.},
	urldate = {2021-06-27},
	journal = {arXiv:1505.05424 [cs, stat]},
	author = {Blundell, Charles and Cornebise, Julien and Kavukcuoglu, Koray and Wierstra, Daan},
	month = may,
	year = {2015},
	note = {arXiv: 1505.05424},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv.org Snapshot:/home/ludvig/Zotero/storage/XH8FI6E8/1505.html:text/html;arXiv Fulltext PDF:/home/ludvig/Zotero/storage/38FDN6H4/Blundell et al. - 2015 - Weight Uncertainty in Neural Networks.pdf:application/pdf},
}

@article{sun_functional_2019,
	title = {Functional {Variational} {Bayesian} {Neural} {Networks}},
	url = {http://arxiv.org/abs/1903.05779},
	abstract = {Variational Bayesian neural networks (BNNs) perform variational inference over weights, but it is difficult to specify meaningful priors and approximate posteriors in a high-dimensional weight space. We introduce functional variational Bayesian neural networks (fBNNs), which maximize an Evidence Lower BOund (ELBO) defined directly on stochastic processes, i.e. distributions over functions. We prove that the KL divergence between stochastic processes equals the supremum of marginal KL divergences over all finite sets of inputs. Based on this, we introduce a practical training objective which approximates the functional ELBO using finite measurement sets and the spectral Stein gradient estimator. With fBNNs, we can specify priors entailing rich structures, including Gaussian processes and implicit stochastic processes. Empirically, we find fBNNs extrapolate well using various structured priors, provide reliable uncertainty estimates, and scale to large datasets.},
	urldate = {2021-06-27},
	journal = {arXiv:1903.05779 [cs, stat]},
	author = {Sun, Shengyang and Zhang, Guodong and Shi, Jiaxin and Grosse, Roger},
	month = mar,
	year = {2019},
	note = {arXiv: 1903.05779},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv.org Snapshot:/home/ludvig/Zotero/storage/WHPGS7US/1903.html:text/html;arXiv Fulltext PDF:/home/ludvig/Zotero/storage/7JMKZBL9/Sun et al. - 2019 - Functional Variational Bayesian Neural Networks.pdf:application/pdf},
}

@article{silver_mastering_2017,
	title = {Mastering the game of {Go} without human knowledge},
	volume = {550},
	copyright = {2017 Macmillan Publishers Limited, part of Springer Nature. All rights reserved.},
	issn = {1476-4687},
	url = {https://www.nature.com/articles/nature24270},
	doi = {10.1038/nature24270},
	abstract = {A long-standing goal of artificial intelligence is an algorithm that learns, tabula rasa, superhuman proficiency in challenging domains. Recently, AlphaGo became the first program to defeat a world champion in the game of Go. The tree search in AlphaGo evaluated positions and selected moves using deep neural networks. These neural networks were trained by supervised learning from human expert moves, and by reinforcement learning from self-play. Here we introduce an algorithm based solely on reinforcement learning, without human data, guidance or domain knowledge beyond game rules. AlphaGo becomes its own teacher: a neural network is trained to predict AlphaGo’s own move selections and also the winner of AlphaGo’s games. This neural network improves the strength of the tree search, resulting in higher quality move selection and stronger self-play in the next iteration. Starting tabula rasa, our new program AlphaGo Zero achieved superhuman performance, winning 100–0 against the previously published, champion-defeating AlphaGo.},
	language = {en},
	number = {7676},
	urldate = {2021-06-17},
	journal = {Nature},
	author = {Silver, David and Schrittwieser, Julian and Simonyan, Karen and Antonoglou, Ioannis and Huang, Aja and Guez, Arthur and Hubert, Thomas and Baker, Lucas and Lai, Matthew and Bolton, Adrian and Chen, Yutian and Lillicrap, Timothy and Hui, Fan and Sifre, Laurent and van den Driessche, George and Graepel, Thore and Hassabis, Demis},
	month = oct,
	year = {2017},
	note = {Number: 7676
Publisher: Nature Publishing Group},
	pages = {354--359},
	file = {Full Text PDF:/home/ludvig/Zotero/storage/DSWT7I66/Silver et al. - 2017 - Mastering the game of Go without human knowledge.pdf:application/pdf;Snapshot:/home/ludvig/Zotero/storage/34XGNTD8/nature24270.html:text/html},
}

@article{silver_mastering_2016,
	title = {Mastering the game of {Go} with deep neural networks and tree search},
	volume = {529},
	copyright = {2016 Nature Publishing Group, a division of Macmillan Publishers Limited. All Rights Reserved.},
	issn = {1476-4687},
	url = {https://www.nature.com/articles/nature16961},
	doi = {10.1038/nature16961},
	abstract = {The game of Go has long been viewed as the most challenging of classic games for artificial intelligence owing to its enormous search space and the difficulty of evaluating board positions and moves. Here we introduce a new approach to computer Go that uses ‘value networks’ to evaluate board positions and ‘policy networks’ to select moves. These deep neural networks are trained by a novel combination of supervised learning from human expert games, and reinforcement learning from games of self-play. Without any lookahead search, the neural networks play Go at the level of state-of-the-art Monte Carlo tree search programs that simulate thousands of random games of self-play. We also introduce a new search algorithm that combines Monte Carlo simulation with value and policy networks. Using this search algorithm, our program AlphaGo achieved a 99.8\% winning rate against other Go programs, and defeated the human European Go champion by 5 games to 0. This is the first time that a computer program has defeated a human professional player in the full-sized game of Go, a feat previously thought to be at least a decade away.},
	language = {en},
	number = {7587},
	urldate = {2021-06-17},
	journal = {Nature},
	author = {Silver, David and Huang, Aja and Maddison, Chris J. and Guez, Arthur and Sifre, Laurent and van den Driessche, George and Schrittwieser, Julian and Antonoglou, Ioannis and Panneershelvam, Veda and Lanctot, Marc and Dieleman, Sander and Grewe, Dominik and Nham, John and Kalchbrenner, Nal and Sutskever, Ilya and Lillicrap, Timothy and Leach, Madeleine and Kavukcuoglu, Koray and Graepel, Thore and Hassabis, Demis},
	month = jan,
	year = {2016},
	note = {Number: 7587
Publisher: Nature Publishing Group},
	pages = {484--489},
	file = {Snapshot:/home/ludvig/Zotero/storage/69BS3ZUQ/nature16961.html:text/html;Full Text PDF:/home/ludvig/Zotero/storage/EYAWX7SS/Silver et al. - 2016 - Mastering the game of Go with deep neural networks.pdf:application/pdf},
}

@article{anthony_thinking_2017,
	title = {Thinking {Fast} and {Slow} with {Deep} {Learning} and {Tree} {Search}},
	url = {http://arxiv.org/abs/1705.08439},
	abstract = {Sequential decision making problems, such as structured prediction, robotic control, and game playing, require a combination of planning policies and generalisation of those plans. In this paper, we present Expert Iteration (ExIt), a novel reinforcement learning algorithm which decomposes the problem into separate planning and generalisation tasks. Planning new policies is performed by tree search, while a deep neural network generalises those plans. Subsequently, tree search is improved by using the neural network policy to guide search, increasing the strength of new plans. In contrast, standard deep Reinforcement Learning algorithms rely on a neural network not only to generalise plans, but to discover them too. We show that ExIt outperforms REINFORCE for training a neural network to play the board game Hex, and our final tree search agent, trained tabula rasa, defeats MoHex 1.0, the most recent Olympiad Champion player to be publicly released.},
	urldate = {2021-06-09},
	journal = {arXiv:1705.08439 [cs]},
	author = {Anthony, Thomas and Tian, Zheng and Barber, David},
	month = dec,
	year = {2017},
	note = {arXiv: 1705.08439},
	keywords = {Computer Science - Artificial Intelligence},
	file = {arXiv.org Snapshot:/home/ludvig/Zotero/storage/TDD2ZR8C/1705.html:text/html;arXiv Fulltext PDF:/home/ludvig/Zotero/storage/E7WL677W/Anthony et al. - 2017 - Thinking Fast and Slow with Deep Learning and Tree.pdf:application/pdf},
}

@article{silver_mastering_2017-1,
	title = {Mastering {Chess} and {Shogi} by {Self}-{Play} with a {General} {Reinforcement} {Learning} {Algorithm}},
	url = {http://arxiv.org/abs/1712.01815},
	abstract = {The game of chess is the most widely-studied domain in the history of artificial intelligence. The strongest programs are based on a combination of sophisticated search techniques, domain-specific adaptations, and handcrafted evaluation functions that have been refined by human experts over several decades. In contrast, the AlphaGo Zero program recently achieved superhuman performance in the game of Go, by tabula rasa reinforcement learning from games of self-play. In this paper, we generalise this approach into a single AlphaZero algorithm that can achieve, tabula rasa, superhuman performance in many challenging domains. Starting from random play, and given no domain knowledge except the game rules, AlphaZero achieved within 24 hours a superhuman level of play in the games of chess and shogi (Japanese chess) as well as Go, and convincingly defeated a world-champion program in each case.},
	urldate = {2021-06-09},
	journal = {arXiv:1712.01815 [cs]},
	author = {Silver, David and Hubert, Thomas and Schrittwieser, Julian and Antonoglou, Ioannis and Lai, Matthew and Guez, Arthur and Lanctot, Marc and Sifre, Laurent and Kumaran, Dharshan and Graepel, Thore and Lillicrap, Timothy and Simonyan, Karen and Hassabis, Demis},
	month = dec,
	year = {2017},
	note = {arXiv: 1712.01815},
	keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence},
	file = {arXiv.org Snapshot:/home/ludvig/Zotero/storage/LR2G9Q4X/1712.html:text/html;arXiv Fulltext PDF:/home/ludvig/Zotero/storage/EXL9I7BM/Silver et al. - 2017 - Mastering Chess and Shogi by Self-Play with a Gene.pdf:application/pdf},
}

@article{houthooft_vime_2017,
	title = {{VIME}: {Variational} {Information} {Maximizing} {Exploration}},
	shorttitle = {{VIME}},
	url = {http://arxiv.org/abs/1605.09674},
	abstract = {Scalable and effective exploration remains a key challenge in reinforcement learning (RL). While there are methods with optimality guarantees in the setting of discrete state and action spaces, these methods cannot be applied in high-dimensional deep RL scenarios. As such, most contemporary RL relies on simple heuristics such as epsilon-greedy exploration or adding Gaussian noise to the controls. This paper introduces Variational Information Maximizing Exploration (VIME), an exploration strategy based on maximization of information gain about the agent's belief of environment dynamics. We propose a practical implementation, using variational inference in Bayesian neural networks which efficiently handles continuous state and action spaces. VIME modifies the MDP reward function, and can be applied with several different underlying RL algorithms. We demonstrate that VIME achieves significantly better performance compared to heuristic exploration methods across a variety of continuous control tasks and algorithms, including tasks with very sparse rewards.},
	urldate = {2021-06-09},
	journal = {arXiv:1605.09674 [cs, stat]},
	author = {Houthooft, Rein and Chen, Xi and Duan, Yan and Schulman, John and De Turck, Filip and Abbeel, Pieter},
	month = jan,
	year = {2017},
	note = {arXiv: 1605.09674},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Artificial Intelligence, Computer Science - Robotics},
	file = {arXiv.org Snapshot:/home/ludvig/Zotero/storage/RSPHEVSX/1605.html:text/html;arXiv Fulltext PDF:/home/ludvig/Zotero/storage/A78HR6DZ/Houthooft et al. - 2017 - VIME Variational Information Maximizing Explorati.pdf:application/pdf},
}

@article{liu_action-depedent_2018,
	title = {Action-depedent {Control} {Variates} for {Policy} {Optimization} via {Stein}'s {Identity}},
	url = {http://arxiv.org/abs/1710.11198},
	abstract = {Policy gradient methods have achieved remarkable successes in solving challenging reinforcement learning problems. However, it still often suffers from the large variance issue on policy gradient estimation, which leads to poor sample efficiency during training. In this work, we propose a control variate method to effectively reduce variance for policy gradient methods. Motivated by the Stein's identity, our method extends the previous control variate methods used in REINFORCE and advantage actor-critic by introducing more general action-dependent baseline functions. Empirical studies show that our method significantly improves the sample efficiency of the state-of-the-art policy gradient approaches.},
	urldate = {2021-06-09},
	journal = {arXiv:1710.11198 [cs, stat]},
	author = {Liu, Hao and Feng, Yihao and Mao, Yi and Zhou, Dengyong and Peng, Jian and Liu, Qiang},
	month = feb,
	year = {2018},
	note = {arXiv: 1710.11198},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv.org Snapshot:/home/ludvig/Zotero/storage/U8YMSHU9/1710.html:text/html;arXiv Fulltext PDF:/home/ludvig/Zotero/storage/2SM3SAU2/Liu et al. - 2018 - Action-depedent Control Variates for Policy Optimi.pdf:application/pdf},
}

@article{lillicrap_continuous_2019,
	title = {Continuous control with deep reinforcement learning},
	url = {http://arxiv.org/abs/1509.02971},
	abstract = {We adapt the ideas underlying the success of Deep Q-Learning to the continuous action domain. We present an actor-critic, model-free algorithm based on the deterministic policy gradient that can operate over continuous action spaces. Using the same learning algorithm, network architecture and hyper-parameters, our algorithm robustly solves more than 20 simulated physics tasks, including classic problems such as cartpole swing-up, dexterous manipulation, legged locomotion and car driving. Our algorithm is able to find policies whose performance is competitive with those found by a planning algorithm with full access to the dynamics of the domain and its derivatives. We further demonstrate that for many of the tasks the algorithm can learn policies end-to-end: directly from raw pixel inputs.},
	urldate = {2021-06-09},
	journal = {arXiv:1509.02971 [cs, stat]},
	author = {Lillicrap, Timothy P. and Hunt, Jonathan J. and Pritzel, Alexander and Heess, Nicolas and Erez, Tom and Tassa, Yuval and Silver, David and Wierstra, Daan},
	month = jul,
	year = {2019},
	note = {arXiv: 1509.02971},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv.org Snapshot:/home/ludvig/Zotero/storage/US8QWL9W/1509.html:text/html;arXiv Fulltext PDF:/home/ludvig/Zotero/storage/LLVL3LH2/Lillicrap et al. - 2019 - Continuous control with deep reinforcement learnin.pdf:application/pdf},
}

@article{haarnoja_soft_2018,
	title = {Soft {Actor}-{Critic}: {Off}-{Policy} {Maximum} {Entropy} {Deep} {Reinforcement} {Learning} with a {Stochastic} {Actor}},
	shorttitle = {Soft {Actor}-{Critic}},
	url = {http://arxiv.org/abs/1801.01290},
	abstract = {Model-free deep reinforcement learning (RL) algorithms have been demonstrated on a range of challenging decision making and control tasks. However, these methods typically suffer from two major challenges: very high sample complexity and brittle convergence properties, which necessitate meticulous hyperparameter tuning. Both of these challenges severely limit the applicability of such methods to complex, real-world domains. In this paper, we propose soft actor-critic, an off-policy actor-critic deep RL algorithm based on the maximum entropy reinforcement learning framework. In this framework, the actor aims to maximize expected reward while also maximizing entropy. That is, to succeed at the task while acting as randomly as possible. Prior deep RL methods based on this framework have been formulated as Q-learning methods. By combining off-policy updates with a stable stochastic actor-critic formulation, our method achieves state-of-the-art performance on a range of continuous control benchmark tasks, outperforming prior on-policy and off-policy methods. Furthermore, we demonstrate that, in contrast to other off-policy algorithms, our approach is very stable, achieving very similar performance across different random seeds.},
	urldate = {2021-06-09},
	journal = {arXiv:1801.01290 [cs, stat]},
	author = {Haarnoja, Tuomas and Zhou, Aurick and Abbeel, Pieter and Levine, Sergey},
	month = aug,
	year = {2018},
	note = {arXiv: 1801.01290},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Artificial Intelligence},
	file = {arXiv.org Snapshot:/home/ludvig/Zotero/storage/CSBNK793/1801.html:text/html;arXiv Fulltext PDF:/home/ludvig/Zotero/storage/T3HGWH3T/Haarnoja et al. - 2018 - Soft Actor-Critic Off-Policy Maximum Entropy Deep.pdf:application/pdf},
}

@article{schulman_trust_2017,
	title = {Trust {Region} {Policy} {Optimization}},
	url = {http://arxiv.org/abs/1502.05477},
	abstract = {We describe an iterative procedure for optimizing policies, with guaranteed monotonic improvement. By making several approximations to the theoretically-justified procedure, we develop a practical algorithm, called Trust Region Policy Optimization (TRPO). This algorithm is similar to natural policy gradient methods and is effective for optimizing large nonlinear policies such as neural networks. Our experiments demonstrate its robust performance on a wide variety of tasks: learning simulated robotic swimming, hopping, and walking gaits; and playing Atari games using images of the screen as input. Despite its approximations that deviate from the theory, TRPO tends to give monotonic improvement, with little tuning of hyperparameters.},
	urldate = {2021-06-09},
	journal = {arXiv:1502.05477 [cs]},
	author = {Schulman, John and Levine, Sergey and Moritz, Philipp and Jordan, Michael I. and Abbeel, Pieter},
	month = apr,
	year = {2017},
	note = {arXiv: 1502.05477},
	keywords = {Computer Science - Machine Learning},
	file = {arXiv.org Snapshot:/home/ludvig/Zotero/storage/PRADPPCG/1502.html:text/html;arXiv Fulltext PDF:/home/ludvig/Zotero/storage/8XIQZWI3/Schulman et al. - 2017 - Trust Region Policy Optimization.pdf:application/pdf},
}

@article{van_hasselt_deep_2015,
	title = {Deep {Reinforcement} {Learning} with {Double} {Q}-learning},
	url = {http://arxiv.org/abs/1509.06461},
	abstract = {The popular Q-learning algorithm is known to overestimate action values under certain conditions. It was not previously known whether, in practice, such overestimations are common, whether they harm performance, and whether they can generally be prevented. In this paper, we answer all these questions affirmatively. In particular, we first show that the recent DQN algorithm, which combines Q-learning with a deep neural network, suffers from substantial overestimations in some games in the Atari 2600 domain. We then show that the idea behind the Double Q-learning algorithm, which was introduced in a tabular setting, can be generalized to work with large-scale function approximation. We propose a specific adaptation to the DQN algorithm and show that the resulting algorithm not only reduces the observed overestimations, as hypothesized, but that this also leads to much better performance on several games.},
	urldate = {2021-06-09},
	journal = {arXiv:1509.06461 [cs]},
	author = {van Hasselt, Hado and Guez, Arthur and Silver, David},
	month = dec,
	year = {2015},
	note = {arXiv: 1509.06461},
	keywords = {Computer Science - Machine Learning},
	file = {arXiv.org Snapshot:/home/ludvig/Zotero/storage/E42YDS5I/1509.html:text/html;arXiv Fulltext PDF:/home/ludvig/Zotero/storage/2ITB7GYP/van Hasselt et al. - 2015 - Deep Reinforcement Learning with Double Q-learning.pdf:application/pdf},
}

@article{wang_dueling_2016,
	title = {Dueling {Network} {Architectures} for {Deep} {Reinforcement} {Learning}},
	url = {http://arxiv.org/abs/1511.06581},
	abstract = {In recent years there have been many successes of using deep representations in reinforcement learning. Still, many of these applications use conventional architectures, such as convolutional networks, LSTMs, or auto-encoders. In this paper, we present a new neural network architecture for model-free reinforcement learning. Our dueling network represents two separate estimators: one for the state value function and one for the state-dependent action advantage function. The main benefit of this factoring is to generalize learning across actions without imposing any change to the underlying reinforcement learning algorithm. Our results show that this architecture leads to better policy evaluation in the presence of many similar-valued actions. Moreover, the dueling architecture enables our RL agent to outperform the state-of-the-art on the Atari 2600 domain.},
	urldate = {2021-06-09},
	journal = {arXiv:1511.06581 [cs]},
	author = {Wang, Ziyu and Schaul, Tom and Hessel, Matteo and van Hasselt, Hado and Lanctot, Marc and de Freitas, Nando},
	month = apr,
	year = {2016},
	note = {arXiv: 1511.06581},
	keywords = {Computer Science - Machine Learning},
	file = {arXiv.org Snapshot:/home/ludvig/Zotero/storage/Y3RLECIK/1511.html:text/html;arXiv Fulltext PDF:/home/ludvig/Zotero/storage/SFV48L7N/Wang et al. - 2016 - Dueling Network Architectures for Deep Reinforceme.pdf:application/pdf},
}

@article{mnih_playing_nodate,
	title = {Playing {Atari} with {Deep} {Reinforcement} {Learning}},
	abstract = {We present the ﬁrst deep learning model to successfully learn control policies directly from high-dimensional sensory input using reinforcement learning. The model is a convolutional neural network, trained with a variant of Q-learning, whose input is raw pixels and whose output is a value function estimating future rewards. We apply our method to seven Atari 2600 games from the Arcade Learning Environment, with no adjustment of the architecture or learning algorithm. We ﬁnd that it outperforms all previous approaches on six of the games and surpasses a human expert on three of them.},
	language = {en},
	author = {Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Graves, Alex and Antonoglou, Ioannis and Wierstra, Daan and Riedmiller, Martin},
	pages = {9},
	file = {Mnih et al. - Playing Atari with Deep Reinforcement Learning.pdf:/home/ludvig/Zotero/storage/Z8IE9NP6/Mnih et al. - Playing Atari with Deep Reinforcement Learning.pdf:application/pdf},
}

@article{schulman_high-dimensional_2018,
	title = {High-{Dimensional} {Continuous} {Control} {Using} {Generalized} {Advantage} {Estimation}},
	url = {http://arxiv.org/abs/1506.02438},
	abstract = {Policy gradient methods are an appealing approach in reinforcement learning because they directly optimize the cumulative reward and can straightforwardly be used with nonlinear function approximators such as neural networks. The two main challenges are the large number of samples typically required, and the difficulty of obtaining stable and steady improvement despite the nonstationarity of the incoming data. We address the first challenge by using value functions to substantially reduce the variance of policy gradient estimates at the cost of some bias, with an exponentially-weighted estimator of the advantage function that is analogous to TD(lambda). We address the second challenge by using trust region optimization procedure for both the policy and the value function, which are represented by neural networks. Our approach yields strong empirical results on highly challenging 3D locomotion tasks, learning running gaits for bipedal and quadrupedal simulated robots, and learning a policy for getting the biped to stand up from starting out lying on the ground. In contrast to a body of prior work that uses hand-crafted policy representations, our neural network policies map directly from raw kinematics to joint torques. Our algorithm is fully model-free, and the amount of simulated experience required for the learning tasks on 3D bipeds corresponds to 1-2 weeks of real time.},
	urldate = {2021-06-09},
	journal = {arXiv:1506.02438 [cs]},
	author = {Schulman, John and Moritz, Philipp and Levine, Sergey and Jordan, Michael and Abbeel, Pieter},
	month = oct,
	year = {2018},
	note = {arXiv: 1506.02438},
	keywords = {Computer Science - Machine Learning, Computer Science - Robotics, Electrical Engineering and Systems Science - Systems and Control},
	file = {arXiv.org Snapshot:/home/ludvig/Zotero/storage/EWIWPW3B/1506.html:text/html;arXiv Fulltext PDF:/home/ludvig/Zotero/storage/XP228ZS7/Schulman et al. - 2018 - High-Dimensional Continuous Control Using Generali.pdf:application/pdf},
}

@article{lecun_mnist_2010-2,
	title = {{MNIST} handwritten digit database},
	volume = {2},
	journal = {ATT Labs [Online]. Available: http://yann.lecun.com/exdb/mnist},
	author = {LeCun, Yann and Cortes, Corinna and Burges, CJ},
	year = {2010},
}

@misc{noauthor_zotero_nodate,
	title = {Zotero {\textbar} {Downloads}},
	url = {https://www.zotero.org/download/},
	urldate = {2021-06-28},
}

@article{mnih_playing_2013,
	title = {Playing {Atari} with {Deep} {Reinforcement} {Learning}},
	url = {http://arxiv.org/abs/1312.5602},
	abstract = {We present the first deep learning model to successfully learn control policies directly from high-dimensional sensory input using reinforcement learning. The model is a convolutional neural network, trained with a variant of Q-learning, whose input is raw pixels and whose output is a value function estimating future rewards. We apply our method to seven Atari 2600 games from the Arcade Learning Environment, with no adjustment of the architecture or learning algorithm. We find that it outperforms all previous approaches on six of the games and surpasses a human expert on three of them.},
	urldate = {2021-06-28},
	journal = {arXiv:1312.5602 [cs]},
	author = {Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Graves, Alex and Antonoglou, Ioannis and Wierstra, Daan and Riedmiller, Martin},
	month = dec,
	year = {2013},
	note = {arXiv: 1312.5602},
	keywords = {Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:/home/ludvig/Zotero/storage/5ULW4PZT/Mnih et al. - 2013 - Playing Atari with Deep Reinforcement Learning.pdf:application/pdf;arXiv.org Snapshot:/home/ludvig/Zotero/storage/GC2QWPVP/1312.html:text/html},
}

@article{ostrovski_count-based_2017,
	title = {Count-{Based} {Exploration} with {Neural} {Density} {Models}},
	url = {http://arxiv.org/abs/1703.01310},
	abstract = {Bellemare et al. (2016) introduced the notion of a pseudo-count, derived from a density model, to generalize count-based exploration to non-tabular reinforcement learning. This pseudo-count was used to generate an exploration bonus for a DQN agent and combined with a mixed Monte Carlo update was sufficient to achieve state of the art on the Atari 2600 game Montezuma's Revenge. We consider two questions left open by their work: First, how important is the quality of the density model for exploration? Second, what role does the Monte Carlo update play in exploration? We answer the first question by demonstrating the use of PixelCNN, an advanced neural density model for images, to supply a pseudo-count. In particular, we examine the intrinsic difficulties in adapting Bellemare et al.'s approach when assumptions about the model are violated. The result is a more practical and general algorithm requiring no special apparatus. We combine PixelCNN pseudo-counts with different agent architectures to dramatically improve the state of the art on several hard Atari games. One surprising finding is that the mixed Monte Carlo update is a powerful facilitator of exploration in the sparsest of settings, including Montezuma's Revenge.},
	urldate = {2021-09-08},
	journal = {arXiv:1703.01310 [cs]},
	author = {Ostrovski, Georg and Bellemare, Marc G. and Oord, Aaron van den and Munos, Remi},
	month = jun,
	year = {2017},
	note = {arXiv: 1703.01310},
	keywords = {Computer Science - Artificial Intelligence},
	file = {arXiv Fulltext PDF:/home/ludvig/Zotero/storage/FYR4SH7D/Ostrovski et al. - 2017 - Count-Based Exploration with Neural Density Models.pdf:application/pdf;arXiv.org Snapshot:/home/ludvig/Zotero/storage/AYR6RTN8/1703.html:text/html},
}

@article{zhao_curiosity-driven_2020,
	title = {Curiosity-{Driven} {Experience} {Prioritization} via {Density} {Estimation}},
	url = {http://arxiv.org/abs/1902.08039},
	abstract = {In Reinforcement Learning (RL), an agent explores the environment and collects trajectories into the memory buffer for later learning. However, the collected trajectories can easily be imbalanced with respect to the achieved goal states. The problem of learning from imbalanced data is a well-known problem in supervised learning, but has not yet been thoroughly researched in RL. To address this problem, we propose a novel Curiosity-Driven Prioritization (CDP) framework to encourage the agent to over-sample those trajectories that have rare achieved goal states. The CDP framework mimics the human learning process and focuses more on relatively uncommon events. We evaluate our methods using the robotic environment provided by OpenAI Gym. The environment contains six robot manipulation tasks. In our experiments, we combined CDP with Deep Deterministic Policy Gradient (DDPG) with or without Hindsight Experience Replay (HER). The experimental results show that CDP improves both performance and sample-efficiency of reinforcement learning agents, compared to state-of-the-art methods.},
	urldate = {2021-09-08},
	journal = {arXiv:1902.08039 [cs, stat]},
	author = {Zhao, Rui and Tresp, Volker},
	month = may,
	year = {2020},
	note = {arXiv: 1902.08039},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Artificial Intelligence},
	file = {arXiv Fulltext PDF:/home/ludvig/Zotero/storage/8VNZ44UF/Zhao and Tresp - 2020 - Curiosity-Driven Experience Prioritization via Den.pdf:application/pdf;arXiv.org Snapshot:/home/ludvig/Zotero/storage/4MTCUCIB/1902.html:text/html},
}

@article{tang_exploration_2017,
	title = {\#{Exploration}: {A} {Study} of {Count}-{Based} {Exploration} for {Deep} {Reinforcement} {Learning}},
	shorttitle = {\#{Exploration}},
	url = {http://arxiv.org/abs/1611.04717},
	abstract = {Count-based exploration algorithms are known to perform near-optimally when used in conjunction with tabular reinforcement learning (RL) methods for solving small discrete Markov decision processes (MDPs). It is generally thought that count-based methods cannot be applied in high-dimensional state spaces, since most states will only occur once. Recent deep RL exploration strategies are able to deal with high-dimensional continuous state spaces through complex heuristics, often relying on optimism in the face of uncertainty or intrinsic motivation. In this work, we describe a surprising finding: a simple generalization of the classic count-based approach can reach near state-of-the-art performance on various high-dimensional and/or continuous deep RL benchmarks. States are mapped to hash codes, which allows to count their occurrences with a hash table. These counts are then used to compute a reward bonus according to the classic count-based exploration theory. We find that simple hash functions can achieve surprisingly good results on many challenging tasks. Furthermore, we show that a domain-dependent learned hash code may further improve these results. Detailed analysis reveals important aspects of a good hash function: 1) having appropriate granularity and 2) encoding information relevant to solving the MDP. This exploration strategy achieves near state-of-the-art performance on both continuous control tasks and Atari 2600 games, hence providing a simple yet powerful baseline for solving MDPs that require considerable exploration.},
	urldate = {2021-09-08},
	journal = {arXiv:1611.04717 [cs]},
	author = {Tang, Haoran and Houthooft, Rein and Foote, Davis and Stooke, Adam and Chen, Xi and Duan, Yan and Schulman, John and De Turck, Filip and Abbeel, Pieter},
	month = dec,
	year = {2017},
	note = {arXiv: 1611.04717},
	keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence},
	file = {arXiv Fulltext PDF:/home/ludvig/Zotero/storage/RHHW64W6/Tang et al. - 2017 - #Exploration A Study of Count-Based Exploration f.pdf:application/pdf;arXiv.org Snapshot:/home/ludvig/Zotero/storage/VL4QTHLF/1611.html:text/html},
}

@article{osband_deep_2016,
	title = {Deep {Exploration} via {Bootstrapped} {DQN}},
	url = {http://arxiv.org/abs/1602.04621},
	abstract = {Efficient exploration in complex environments remains a major challenge for reinforcement learning. We propose bootstrapped DQN, a simple algorithm that explores in a computationally and statistically efficient manner through use of randomized value functions. Unlike dithering strategies such as epsilon-greedy exploration, bootstrapped DQN carries out temporally-extended (or deep) exploration; this can lead to exponentially faster learning. We demonstrate these benefits in complex stochastic MDPs and in the large-scale Arcade Learning Environment. Bootstrapped DQN substantially improves learning times and performance across most Atari games.},
	urldate = {2021-09-08},
	journal = {arXiv:1602.04621 [cs, stat]},
	author = {Osband, Ian and Blundell, Charles and Pritzel, Alexander and Van Roy, Benjamin},
	month = jul,
	year = {2016},
	note = {arXiv: 1602.04621},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Artificial Intelligence, Electrical Engineering and Systems Science - Systems and Control},
	file = {arXiv Fulltext PDF:/home/ludvig/Zotero/storage/VLIHHRVI/Osband et al. - 2016 - Deep Exploration via Bootstrapped DQN.pdf:application/pdf;arXiv.org Snapshot:/home/ludvig/Zotero/storage/KENKJHT2/1602.html:text/html},
}

@inproceedings{osband_more_2013,
	title = {({More}) {Efficient} {Reinforcement} {Learning} via {Posterior} {Sampling}},
	volume = {26},
	url = {https://proceedings.neurips.cc/paper/2013/hash/6a5889bb0190d0211a991f47bb19a777-Abstract.html},
	urldate = {2021-10-28},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Osband, Ian and Russo, Daniel and Van Roy, Benjamin},
	year = {2013},
	file = {Full Text PDF:/home/ludvig/Zotero/storage/PGSG2ZT4/Osband et al. - 2013 - (More) Efficient Reinforcement Learning via Poster.pdf:application/pdf},
}

@article{odonoghue_uncertainty_2018,
	title = {The {Uncertainty} {Bellman} {Equation} and {Exploration}},
	url = {http://arxiv.org/abs/1709.05380},
	abstract = {We consider the exploration/exploitation problem in reinforcement learning. For exploitation, it is well known that the Bellman equation connects the value at any time-step to the expected value at subsequent time-steps. In this paper we consider a similar uncertainty Bellman equation (UBE), which connects the uncertainty at any time-step to the expected uncertainties at subsequent time-steps, thereby extending the potential exploratory beneﬁt of a policy beyond individual time-steps. We prove that the unique ﬁxed point of the UBE yields an upper bound on the variance of the posterior distribution of the Q-values induced by any policy. This bound can be much tighter than traditional count-based bonuses that compound standard deviation rather than variance. Importantly, and unlike several existing approaches to optimism, this method scales naturally to large systems with complex generalization. Substituting our UBE-exploration strategy for -greedy improves DQN performance on 51 out of 57 games in the Atari suite.},
	language = {en},
	urldate = {2021-10-28},
	journal = {arXiv:1709.05380 [cs, math, stat]},
	author = {O'Donoghue, Brendan and Osband, Ian and Munos, Remi and Mnih, Volodymyr},
	month = oct,
	year = {2018},
	note = {arXiv: 1709.05380},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Artificial Intelligence, Mathematics - Optimization and Control},
	file = {O'Donoghue et al. - 2018 - The Uncertainty Bellman Equation and Exploration.pdf:/home/ludvig/Zotero/storage/5MWSY3TP/O'Donoghue et al. - 2018 - The Uncertainty Bellman Equation and Exploration.pdf:application/pdf},
}

@article{dearden_bayesian_nodate,
	title = {Bayesian {Q}-{Learning}},
	abstract = {A central problem in learning in complex environments is balancing exploration of untested actions against exploitation of actions that are known to be good. The beneﬁt of exploration can be estimated using the classical notion of Value of Information—the expected improvement in future decision quality that might arise from the information acquired by exploration. Estimating this quantity requires an assessment of the agent’s uncertainty about its current value estimates for states. In this paper, we adopt a Bayesian approach to maintaining this uncertain information. We extend Watkins’ Q-learning by maintaining and propagating probability distributions over the Q-values. These distributions are used to compute a myopic approximation to the value of information for each action and hence to select the action that best balances exploration and exploitation. We establish the convergence properties of our algorithm and show experimentally that it can exhibit substantial improvements over other well-known model-free exploration strategies.},
	language = {en},
	author = {Dearden, Richard and Friedman, Nir and Russell, Stuart},
	pages = {8},
	file = {Dearden et al. - Bayesian Q-Learning.pdf:/home/ludvig/Zotero/storage/7NGS3ZD7/Dearden et al. - Bayesian Q-Learning.pdf:application/pdf},
}

@article{fellows_bayesian_2021,
	title = {Bayesian {Bellman} {Operators}},
	url = {http://arxiv.org/abs/2106.05012},
	abstract = {We introduce a novel perspective on Bayesian reinforcement learning (RL); whereas existing approaches infer a posterior over the transition distribution or Q-function, we characterise the uncertainty in the Bellman operator. Our Bayesian Bellman operator (BBO) framework is motivated by the insight that when bootstrapping is introduced, model-free approaches actually infer a posterior over Bellman operators, not value functions. In this paper, we use BBO to provide a rigorous theoretical analysis of model-free Bayesian RL to better understand its relationship to established frequentist RL methodologies. We prove that Bayesian solutions are consistent with frequentist RL solutions, even when approximate inference is used, and derive conditions for which convergence properties hold. Empirically, we demonstrate that algorithms derived from the BBO framework have sophisticated deep exploration properties that enable them to solve continuous control tasks at which state-of-the-art regularised actor-critic algorithms fail catastrophically.},
	language = {en},
	urldate = {2021-10-28},
	journal = {arXiv:2106.05012 [cs]},
	author = {Fellows, Matthew and Hartikainen, Kristian and Whiteson, Shimon},
	month = jun,
	year = {2021},
	note = {arXiv: 2106.05012},
	keywords = {Computer Science - Machine Learning},
	file = {Fellows et al. - 2021 - Bayesian Bellman Operators.pdf:/home/ludvig/Zotero/storage/874CC8RB/Fellows et al. - 2021 - Bayesian Bellman Operators.pdf:application/pdf},
}

@article{riquelme_deep_2018,
	title = {{DEEP} {BAYESIAN} {BANDITS} {SHOWDOWN}},
	abstract = {Recent advances in deep reinforcement learning have made signiﬁcant strides in performance on applications such as Go and Atari games. However, developing practical methods to balance exploration and exploitation in complex domains remains largely unsolved. Thompson Sampling and its extension to reinforcement learning provide an elegant approach to exploration that only requires access to posterior samples of the model. At the same time, advances in approximate Bayesian methods have made posterior approximation for ﬂexible neural network models practical. Thus, it is attractive to consider approximate Bayesian neural networks in a Thompson Sampling framework. To understand the impact of using an approximate posterior on Thompson Sampling, we benchmark well-established and recently developed methods for approximate posterior sampling combined with Thompson Sampling over a series of contextual bandit problems. We found that many approaches that have been successful in the supervised learning setting underperformed in the sequential decision-making scenario. In particular, we highlight the challenge of adapting slowly converging uncertainty estimates to the online setting.},
	language = {en},
	author = {Riquelme, Carlos and Tucker, George and Snoek, Jasper},
	year = {2018},
	pages = {27},
	file = {Riquelme et al. - 2018 - DEEP BAYESIAN BANDITS SHOWDOWN.pdf:/home/ludvig/Zotero/storage/BSYYB96Z/Riquelme et al. - 2018 - DEEP BAYESIAN BANDITS SHOWDOWN.pdf:application/pdf},
}

@article{osband_randomized_2018,
	title = {Randomized {Prior} {Functions} for {Deep} {Reinforcement} {Learning}},
	url = {http://arxiv.org/abs/1806.03335},
	abstract = {Dealing with uncertainty is essential for eﬃcient reinforcement learning. There is a growing literature on uncertainty estimation for deep learning from ﬁxed datasets, but many of the most popular approaches are poorlysuited to sequential decision problems. Other methods, such as bootstrap sampling, have no mechanism for uncertainty that does not come from the observed data. We highlight why this can be a crucial shortcoming and propose a simple remedy through addition of a randomized untrainable ‘prior’ network to each ensemble member. We prove that this approach is eﬃcient with linear representations, provide simple illustrations of its eﬃcacy with nonlinear representations and show that this approach scales to large-scale problems far better than previous attempts.},
	language = {en},
	urldate = {2021-10-28},
	journal = {arXiv:1806.03335 [cs, stat]},
	author = {Osband, Ian and Aslanides, John and Cassirer, Albin},
	month = nov,
	year = {2018},
	note = {arXiv: 1806.03335},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Artificial Intelligence},
	file = {Osband et al. - 2018 - Randomized Prior Functions for Deep Reinforcement .pdf:/home/ludvig/Zotero/storage/QGX8T5FR/Osband et al. - 2018 - Randomized Prior Functions for Deep Reinforcement .pdf:application/pdf},
}

@article{osband_deep_2019,
	title = {Deep {Exploration} via {Randomized} {Value} {Functions}},
	url = {http://arxiv.org/abs/1703.07608},
	abstract = {We study the use of randomized value functions to guide deep exploration in reinforcement learning. This oﬀers an elegant means for synthesizing statistically and computationally eﬃcient exploration with common practical approaches to value function learning. We present several reinforcement learning algorithms that leverage randomized value functions and demonstrate their eﬃcacy through computational studies. We also prove a regret bound that establishes statistical eﬃciency with a tabular representation.},
	language = {en},
	urldate = {2021-10-28},
	journal = {arXiv:1703.07608 [cs, stat]},
	author = {Osband, Ian and Van Roy, Benjamin and Russo, Daniel and Wen, Zheng},
	month = sep,
	year = {2019},
	note = {arXiv: 1703.07608},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Artificial Intelligence},
	file = {Osband et al. - 2019 - Deep Exploration via Randomized Value Functions.pdf:/home/ludvig/Zotero/storage/7UZW8V2P/Osband et al. - 2019 - Deep Exploration via Randomized Value Functions.pdf:application/pdf},
}

@article{ghavamzadeh_bayesian_2015,
	title = {Bayesian {Reinforcement} {Learning}: {A} {Survey}},
	volume = {8},
	issn = {1935-8237, 1935-8245},
	shorttitle = {Bayesian {Reinforcement} {Learning}},
	url = {http://arxiv.org/abs/1609.04436},
	doi = {10.1561/2200000049},
	abstract = {Bayesian methods for machine learning have been widely investigated, yielding principled methods for incorporating prior information into inference algorithms. In this survey, we provide an in-depth review of the role of Bayesian methods for the reinforcement learning (RL) paradigm. The major incentives for incorporating Bayesian reasoning in RL are: 1) it provides an elegant approach to action-selection (exploration/exploitation) as a function of the uncertainty in learning; and 2) it provides a machinery to incorporate prior knowledge into the algorithms. We first discuss models and methods for Bayesian inference in the simple single-step Bandit model. We then review the extensive recent literature on Bayesian methods for model-based RL, where prior information can be expressed on the parameters of the Markov model. We also present Bayesian methods for model-free RL, where priors are expressed over the value function or policy class. The objective of the paper is to provide a comprehensive survey on Bayesian RL algorithms and their theoretical and empirical properties.},
	language = {en},
	number = {5-6},
	urldate = {2021-11-05},
	journal = {Foundations and Trends® in Machine Learning},
	author = {Ghavamzadeh, Mohammad and Mannor, Shie and Pineau, Joelle and Tamar, Aviv},
	year = {2015},
	note = {arXiv: 1609.04436},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Artificial Intelligence},
	pages = {359--483},
	file = {Ghavamzadeh et al. - 2015 - Bayesian Reinforcement Learning A Survey.pdf:/home/ludvig/Zotero/storage/GCU5BZM7/Ghavamzadeh et al. - 2015 - Bayesian Reinforcement Learning A Survey.pdf:application/pdf},
}

@book{wiering_reinforcement_2012,
	address = {Berlin, Heidelberg},
	series = {Adaptation, {Learning}, and {Optimization}},
	title = {Reinforcement {Learning}},
	volume = {12},
	isbn = {978-3-642-27644-6 978-3-642-27645-3},
	url = {http://link.springer.com/10.1007/978-3-642-27645-3},
	language = {en},
	urldate = {2021-11-05},
	publisher = {Springer Berlin Heidelberg},
	editor = {Wiering, Marco and van Otterlo, Martijn},
	year = {2012},
	doi = {10.1007/978-3-642-27645-3},
	file = {Wiering and van Otterlo - 2012 - Reinforcement Learning.pdf:/home/ludvig/Zotero/storage/YBSDDI8M/Wiering and van Otterlo - 2012 - Reinforcement Learning.pdf:application/pdf},
}

@article{shi_spectral_2018,
	title = {A {Spectral} {Approach} to {Gradient} {Estimation} for {Implicit} {Distributions}},
	url = {http://arxiv.org/abs/1806.02925},
	abstract = {Recently there have been increasing interests in learning and inference with implicit distributions (i.e., distributions without tractable densities). To this end, we develop a gradient estimator for implicit distributions based on Stein’s identity and a spectral decomposition of kernel operators, where the eigenfunctions are approximated by the Nystro¨m method. Unlike the previous works that only provide estimates at the sample points, our approach directly estimates the gradient function, thus allows for a simple and principled out-ofsample extension. We provide theoretical results on the error bound of the estimator and discuss the bias-variance tradeoff in practice. The effectiveness of our method is demonstrated by applications to gradient-free Hamiltonian Monte Carlo and variational inference with implicit distributions. Finally, we discuss the intuition behind the estimator by drawing connections between the Nystro¨m method and kernel PCA, which indicates that the estimator can automatically adapt to the geometry of the underlying distribution.},
	language = {en},
	urldate = {2021-11-08},
	journal = {arXiv:1806.02925 [cs, stat]},
	author = {Shi, Jiaxin and Sun, Shengyang and Zhu, Jun},
	month = jun,
	year = {2018},
	note = {arXiv: 1806.02925},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Neural and Evolutionary Computing},
	file = {Shi et al. - 2018 - A Spectral Approach to Gradient Estimation for Imp.pdf:/home/ludvig/Zotero/storage/GAF65C3N/Shi et al. - 2018 - A Spectral Approach to Gradient Estimation for Imp.pdf:application/pdf},
}

@article{shi_spectral_2018-1,
	title = {A {Spectral} {Approach} to {Gradient} {Estimation} for {Implicit} {Distributions}},
	url = {https://arxiv.org/abs/1806.02925v1},
	abstract = {Recently there have been increasing interests in learning and inference with implicit distributions (i.e., distributions without tractable densities). To this end, we develop a gradient estimator for implicit distributions based on Stein's identity and a spectral decomposition of kernel operators, where the eigenfunctions are approximated by the Nystr{\textbackslash}"om method. Unlike the previous works that only provide estimates at the sample points, our approach directly estimates the gradient function, thus allows for a simple and principled out-of-sample extension. We provide theoretical results on the error bound of the estimator and discuss the bias-variance tradeoff in practice. The effectiveness of our method is demonstrated by applications to gradient-free Hamiltonian Monte Carlo and variational inference with implicit distributions. Finally, we discuss the intuition behind the estimator by drawing connections between the Nystr{\textbackslash}"om method and kernel PCA, which indicates that the estimator can automatically adapt to the geometry of the underlying distribution.},
	language = {en},
	urldate = {2021-11-08},
	author = {Shi, Jiaxin and Sun, Shengyang and Zhu, Jun},
	month = jun,
	year = {2018},
	file = {Snapshot:/home/ludvig/Zotero/storage/L3BKL58T/1806.html:text/html;Full Text PDF:/home/ludvig/Zotero/storage/JY6MCQC6/Shi et al. - 2018 - A Spectral Approach to Gradient Estimation for Imp.pdf:application/pdf},
}

@article{shi_spectral_2018-2,
	title = {A {Spectral} {Approach} to {Gradient} {Estimation} for {Implicit} {Distributions}},
	url = {http://arxiv.org/abs/1806.02925},
	abstract = {Recently there have been increasing interests in learning and inference with implicit distributions (i.e., distributions without tractable densities). To this end, we develop a gradient estimator for implicit distributions based on Stein’s identity and a spectral decomposition of kernel operators, where the eigenfunctions are approximated by the Nystro¨m method. Unlike the previous works that only provide estimates at the sample points, our approach directly estimates the gradient function, thus allows for a simple and principled out-ofsample extension. We provide theoretical results on the error bound of the estimator and discuss the bias-variance tradeoff in practice. The effectiveness of our method is demonstrated by applications to gradient-free Hamiltonian Monte Carlo and variational inference with implicit distributions. Finally, we discuss the intuition behind the estimator by drawing connections between the Nystro¨m method and kernel PCA, which indicates that the estimator can automatically adapt to the geometry of the underlying distribution.},
	language = {en},
	urldate = {2021-11-08},
	journal = {arXiv:1806.02925 [cs, stat]},
	author = {Shi, Jiaxin and Sun, Shengyang and Zhu, Jun},
	month = jun,
	year = {2018},
	note = {arXiv: 1806.02925},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Neural and Evolutionary Computing},
}

@article{fortunato_noisy_2019,
	title = {Noisy {Networks} for {Exploration}},
	url = {http://arxiv.org/abs/1706.10295},
	abstract = {We introduce NoisyNet, a deep reinforcement learning agent with parametric noise added to its weights, and show that the induced stochasticity of the agent’s policy can be used to aid efﬁcient exploration. The parameters of the noise are learned with gradient descent along with the remaining network weights. NoisyNet is straightforward to implement and adds little computational overhead. We ﬁnd that replacing the conventional exploration heuristics for A3C, DQN and Dueling agents (entropy reward and -greedy respectively) with NoisyNet yields substantially higher scores for a wide range of Atari games, in some cases advancing the agent from sub to super-human performance.},
	language = {en},
	urldate = {2021-12-03},
	journal = {arXiv:1706.10295 [cs, stat]},
	author = {Fortunato, Meire and Azar, Mohammad Gheshlaghi and Piot, Bilal and Menick, Jacob and Osband, Ian and Graves, Alex and Mnih, Vlad and Munos, Remi and Hassabis, Demis and Pietquin, Olivier and Blundell, Charles and Legg, Shane},
	month = jul,
	year = {2019},
	note = {arXiv: 1706.10295},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {Fortunato et al. - 2019 - Noisy Networks for Exploration.pdf:/home/ludvig/Zotero/storage/M6DMZ8JL/Fortunato et al. - 2019 - Noisy Networks for Exploration.pdf:application/pdf},
}

@article{osband_generalization_2016,
	title = {Generalization and {Exploration} via {Randomized} {Value} {Functions}},
	url = {http://arxiv.org/abs/1402.0635},
	abstract = {We propose randomized least-squares value iteration (RLSVI) -- a new reinforcement learning algorithm designed to explore and generalize efficiently via linearly parameterized value functions. We explain why versions of least-squares value iteration that use Boltzmann or epsilon-greedy exploration can be highly inefficient, and we present computational results that demonstrate dramatic efficiency gains enjoyed by RLSVI. Further, we establish an upper bound on the expected regret of RLSVI that demonstrates near-optimality in a tabula rasa learning context. More broadly, our results suggest that randomized value functions offer a promising approach to tackling a critical challenge in reinforcement learning: synthesizing efficient exploration and effective generalization.},
	urldate = {2021-12-10},
	journal = {arXiv:1402.0635 [cs, stat]},
	author = {Osband, Ian and Van Roy, Benjamin and Wen, Zheng},
	month = feb,
	year = {2016},
	note = {arXiv: 1402.0635},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Artificial Intelligence, Electrical Engineering and Systems Science - Systems and Control},
	file = {arXiv.org Snapshot:/home/ludvig/Zotero/storage/2ZJ8WLUK/1402.html:text/html;arXiv Fulltext PDF:/home/ludvig/Zotero/storage/I5ITSDEV/Osband et al. - 2016 - Generalization and Exploration via Randomized Valu.pdf:application/pdf},
}

@article{hessel_rainbow_2017,
	title = {Rainbow: {Combining} {Improvements} in {Deep} {Reinforcement} {Learning}},
	shorttitle = {Rainbow},
	url = {http://arxiv.org/abs/1710.02298},
	abstract = {The deep reinforcement learning community has made several independent improvements to the DQN algorithm. However, it is unclear which of these extensions are complementary and can be fruitfully combined. This paper examines six extensions to the DQN algorithm and empirically studies their combination. Our experiments show that the combination provides state-of-the-art performance on the Atari 2600 benchmark, both in terms of data efficiency and final performance. We also provide results from a detailed ablation study that shows the contribution of each component to overall performance.},
	urldate = {2021-12-14},
	journal = {arXiv:1710.02298 [cs]},
	author = {Hessel, Matteo and Modayil, Joseph and van Hasselt, Hado and Schaul, Tom and Ostrovski, Georg and Dabney, Will and Horgan, Dan and Piot, Bilal and Azar, Mohammad and Silver, David},
	month = oct,
	year = {2017},
	note = {arXiv: 1710.02298},
	keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence},
	file = {arXiv.org Snapshot:/home/ludvig/Zotero/storage/5JWSJVGK/1710.html:text/html;arXiv Fulltext PDF:/home/ludvig/Zotero/storage/QXSL4IM3/Hessel et al. - 2017 - Rainbow Combining Improvements in Deep Reinforcem.pdf:application/pdf},
}

@article{wang_dueling_2016-1,
	title = {Dueling {Network} {Architectures} for {Deep} {Reinforcement} {Learning}},
	url = {http://arxiv.org/abs/1511.06581},
	abstract = {In recent years there have been many successes of using deep representations in reinforcement learning. Still, many of these applications use conventional architectures, such as convolutional networks, LSTMs, or auto-encoders. In this paper, we present a new neural network architecture for model-free reinforcement learning. Our dueling network represents two separate estimators: one for the state value function and one for the state-dependent action advantage function. The main benefit of this factoring is to generalize learning across actions without imposing any change to the underlying reinforcement learning algorithm. Our results show that this architecture leads to better policy evaluation in the presence of many similar-valued actions. Moreover, the dueling architecture enables our RL agent to outperform the state-of-the-art on the Atari 2600 domain.},
	urldate = {2021-12-14},
	journal = {arXiv:1511.06581 [cs]},
	author = {Wang, Ziyu and Schaul, Tom and Hessel, Matteo and van Hasselt, Hado and Lanctot, Marc and de Freitas, Nando},
	month = apr,
	year = {2016},
	note = {arXiv: 1511.06581},
	keywords = {Computer Science - Machine Learning},
	file = {arXiv.org Snapshot:/home/ludvig/Zotero/storage/LDULE24P/1511.html:text/html;arXiv Fulltext PDF:/home/ludvig/Zotero/storage/BZ8ZVAPF/Wang et al. - 2016 - Dueling Network Architectures for Deep Reinforceme.pdf:application/pdf},
}

@article{van_hasselt_deep_2015-1,
	title = {Deep {Reinforcement} {Learning} with {Double} {Q}-learning},
	url = {http://arxiv.org/abs/1509.06461},
	abstract = {The popular Q-learning algorithm is known to overestimate action values under certain conditions. It was not previously known whether, in practice, such overestimations are common, whether they harm performance, and whether they can generally be prevented. In this paper, we answer all these questions affirmatively. In particular, we first show that the recent DQN algorithm, which combines Q-learning with a deep neural network, suffers from substantial overestimations in some games in the Atari 2600 domain. We then show that the idea behind the Double Q-learning algorithm, which was introduced in a tabular setting, can be generalized to work with large-scale function approximation. We propose a specific adaptation to the DQN algorithm and show that the resulting algorithm not only reduces the observed overestimations, as hypothesized, but that this also leads to much better performance on several games.},
	urldate = {2021-12-14},
	journal = {arXiv:1509.06461 [cs]},
	author = {van Hasselt, Hado and Guez, Arthur and Silver, David},
	month = dec,
	year = {2015},
	note = {arXiv: 1509.06461},
	keywords = {Computer Science - Machine Learning},
	file = {arXiv.org Snapshot:/home/ludvig/Zotero/storage/X27F4QMG/1509.html:text/html;arXiv Fulltext PDF:/home/ludvig/Zotero/storage/C5Z2JNDA/van Hasselt et al. - 2015 - Deep Reinforcement Learning with Double Q-learning.pdf:application/pdf},
}

@article{mnih_asynchronous_2016,
	title = {Asynchronous {Methods} for {Deep} {Reinforcement} {Learning}},
	url = {http://arxiv.org/abs/1602.01783},
	abstract = {We propose a conceptually simple and lightweight framework for deep reinforcement learning that uses asynchronous gradient descent for optimization of deep neural network controllers. We present asynchronous variants of four standard reinforcement learning algorithms and show that parallel actor-learners have a stabilizing effect on training allowing all four methods to successfully train neural network controllers. The best performing method, an asynchronous variant of actor-critic, surpasses the current state-of-the-art on the Atari domain while training for half the time on a single multi-core CPU instead of a GPU. Furthermore, we show that asynchronous actor-critic succeeds on a wide variety of continuous motor control problems as well as on a new task of navigating random 3D mazes using a visual input.},
	urldate = {2021-12-14},
	journal = {arXiv:1602.01783 [cs]},
	author = {Mnih, Volodymyr and Badia, Adrià Puigdomènech and Mirza, Mehdi and Graves, Alex and Lillicrap, Timothy P. and Harley, Tim and Silver, David and Kavukcuoglu, Koray},
	month = jun,
	year = {2016},
	note = {arXiv: 1602.01783},
	keywords = {Computer Science - Machine Learning},
	file = {arXiv.org Snapshot:/home/ludvig/Zotero/storage/J8WPHCY7/1602.html:text/html;arXiv Fulltext PDF:/home/ludvig/Zotero/storage/842IUBN8/Mnih et al. - 2016 - Asynchronous Methods for Deep Reinforcement Learni.pdf:application/pdf},
}

@article{schaul_prioritized_2016,
	title = {Prioritized {Experience} {Replay}},
	url = {http://arxiv.org/abs/1511.05952},
	abstract = {Experience replay lets online reinforcement learning agents remember and reuse experiences from the past. In prior work, experience transitions were uniformly sampled from a replay memory. However, this approach simply replays transitions at the same frequency that they were originally experienced, regardless of their significance. In this paper we develop a framework for prioritizing experience, so as to replay important transitions more frequently, and therefore learn more efficiently. We use prioritized experience replay in Deep Q-Networks (DQN), a reinforcement learning algorithm that achieved human-level performance across many Atari games. DQN with prioritized experience replay achieves a new state-of-the-art, outperforming DQN with uniform replay on 41 out of 49 games.},
	urldate = {2021-12-14},
	journal = {arXiv:1511.05952 [cs]},
	author = {Schaul, Tom and Quan, John and Antonoglou, Ioannis and Silver, David},
	month = feb,
	year = {2016},
	note = {arXiv: 1511.05952},
	keywords = {Computer Science - Machine Learning},
	file = {arXiv.org Snapshot:/home/ludvig/Zotero/storage/XFMT6X8U/1511.html:text/html;arXiv Fulltext PDF:/home/ludvig/Zotero/storage/ANALD9C5/Schaul et al. - 2016 - Prioritized Experience Replay.pdf:application/pdf},
}

@article{bellemare_distributional_2017,
	title = {A {Distributional} {Perspective} on {Reinforcement} {Learning}},
	url = {http://arxiv.org/abs/1707.06887},
	abstract = {In this paper we argue for the fundamental importance of the value distribution: the distribution of the random return received by a reinforcement learning agent. This is in contrast to the common approach to reinforcement learning which models the expectation of this return, or value. Although there is an established body of literature studying the value distribution, thus far it has always been used for a specific purpose such as implementing risk-aware behaviour. We begin with theoretical results in both the policy evaluation and control settings, exposing a significant distributional instability in the latter. We then use the distributional perspective to design a new algorithm which applies Bellman's equation to the learning of approximate value distributions. We evaluate our algorithm using the suite of games from the Arcade Learning Environment. We obtain both state-of-the-art results and anecdotal evidence demonstrating the importance of the value distribution in approximate reinforcement learning. Finally, we combine theoretical and empirical evidence to highlight the ways in which the value distribution impacts learning in the approximate setting.},
	urldate = {2021-12-14},
	journal = {arXiv:1707.06887 [cs, stat]},
	author = {Bellemare, Marc G. and Dabney, Will and Munos, Rémi},
	month = jul,
	year = {2017},
	note = {arXiv: 1707.06887},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Artificial Intelligence},
	file = {arXiv.org Snapshot:/home/ludvig/Zotero/storage/UKFPP77R/1707.html:text/html;arXiv Fulltext PDF:/home/ludvig/Zotero/storage/2J4F3TB2/Bellemare et al. - 2017 - A Distributional Perspective on Reinforcement Lear.pdf:application/pdf},
}

@article{osband_generalization_2016-1,
	title = {Generalization and {Exploration} via {Randomized} {Value} {Functions}},
	url = {http://arxiv.org/abs/1402.0635},
	abstract = {We propose randomized least-squares value iteration (RLSVI) -- a new reinforcement learning algorithm designed to explore and generalize efficiently via linearly parameterized value functions. We explain why versions of least-squares value iteration that use Boltzmann or epsilon-greedy exploration can be highly inefficient, and we present computational results that demonstrate dramatic efficiency gains enjoyed by RLSVI. Further, we establish an upper bound on the expected regret of RLSVI that demonstrates near-optimality in a tabula rasa learning context. More broadly, our results suggest that randomized value functions offer a promising approach to tackling a critical challenge in reinforcement learning: synthesizing efficient exploration and effective generalization.},
	urldate = {2021-12-16},
	journal = {arXiv:1402.0635 [cs, stat]},
	author = {Osband, Ian and Van Roy, Benjamin and Wen, Zheng},
	month = feb,
	year = {2016},
	note = {arXiv: 1402.0635},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Artificial Intelligence, Electrical Engineering and Systems Science - Systems and Control},
	file = {arXiv.org Snapshot:/home/ludvig/Zotero/storage/T9ZPHMAP/1402.html:text/html;arXiv Fulltext PDF:/home/ludvig/Zotero/storage/FFC2IY2M/Osband et al. - 2016 - Generalization and Exploration via Randomized Valu.pdf:application/pdf},
}

@article{thompson_likelihood_1933,
	title = {On the {Likelihood} that {One} {Unknown} {Probability} {Exceeds} {Another} in {View} of the {Evidence} of {Two} {Samples}},
	volume = {25},
	issn = {0006-3444},
	url = {https://www.jstor.org/stable/2332286},
	doi = {10.2307/2332286},
	number = {3/4},
	urldate = {2022-01-05},
	journal = {Biometrika},
	author = {Thompson, William R.},
	year = {1933},
	note = {Publisher: [Oxford University Press, Biometrika Trust]},
	pages = {285--294},
}

@article{maddox_simple_2019,
	title = {A {Simple} {Baseline} for {Bayesian} {Uncertainty} in {Deep} {Learning}},
	url = {https://arxiv.org/abs/1902.02476v2},
	abstract = {We propose SWA-Gaussian (SWAG), a simple, scalable, and general purpose approach for uncertainty representation and calibration in deep learning. Stochastic Weight Averaging (SWA), which computes the first moment of stochastic gradient descent (SGD) iterates with a modified learning rate schedule, has recently been shown to improve generalization in deep learning. With SWAG, we fit a Gaussian using the SWA solution as the first moment and a low rank plus diagonal covariance also derived from the SGD iterates, forming an approximate posterior distribution over neural network weights; we then sample from this Gaussian distribution to perform Bayesian model averaging. We empirically find that SWAG approximates the shape of the true posterior, in accordance with results describing the stationary distribution of SGD iterates. Moreover, we demonstrate that SWAG performs well on a wide variety of tasks, including out of sample detection, calibration, and transfer learning, in comparison to many popular alternatives including MC dropout, KFAC Laplace, SGLD, and temperature scaling.},
	language = {en},
	urldate = {2022-01-10},
	author = {Maddox, Wesley and Garipov, Timur and Izmailov, Pavel and Vetrov, Dmitry and Wilson, Andrew Gordon},
	month = feb,
	year = {2019},
	file = {Full Text PDF:/home/ludvig/Zotero/storage/57F5LRHV/Maddox et al. - 2019 - A Simple Baseline for Bayesian Uncertainty in Deep.pdf:application/pdf;Snapshot:/home/ludvig/Zotero/storage/LD7BGWF7/1902.html:text/html},
}

@article{maddox_simple_2019-1,
	title = {A {Simple} {Baseline} for {Bayesian} {Uncertainty} in {Deep} {Learning}},
	url = {http://arxiv.org/abs/1902.02476},
	abstract = {We propose SWA-Gaussian (SWAG), a simple, scalable, and general purpose approach for uncertainty representation and calibration in deep learning. Stochastic Weight Averaging (SWA), which computes the ﬁrst moment of stochastic gradient descent (SGD) iterates with a modiﬁed learning rate schedule, has recently been shown to improve generalization in deep learning. With SWAG, we ﬁt a Gaussian using the SWA solution as the ﬁrst moment and a low rank plus diagonal covariance also derived from the SGD iterates, forming an approximate posterior distribution over neural network weights; we then sample from this Gaussian distribution to perform Bayesian model averaging. We empirically ﬁnd that SWAG approximates the shape of the true posterior, in accordance with results describing the stationary distribution of SGD iterates. Moreover, we demonstrate that SWAG performs well on a wide variety of tasks, including out of sample detection, calibration, and transfer learning, in comparison to many popular alternatives including MC dropout, KFAC Laplace, SGLD, and temperature scaling.},
	language = {en},
	urldate = {2022-01-10},
	journal = {arXiv:1902.02476 [cs, stat]},
	author = {Maddox, Wesley and Garipov, Timur and Izmailov, Pavel and Vetrov, Dmitry and Wilson, Andrew Gordon},
	month = dec,
	year = {2019},
	note = {arXiv: 1902.02476},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {Maddox et al. - 2019 - A Simple Baseline for Bayesian Uncertainty in Deep.pdf:/home/ludvig/Zotero/storage/AU68D7IG/Maddox et al. - 2019 - A Simple Baseline for Bayesian Uncertainty in Deep.pdf:application/pdf},
}

@article{rezende_stochastic_2014,
	title = {Stochastic {Backpropagation} and {Approximate} {Inference} in {Deep} {Generative} {Models}},
	url = {http://arxiv.org/abs/1401.4082},
	abstract = {We marry ideas from deep neural networks and approximate Bayesian inference to derive a generalised class of deep, directed generative models, endowed with a new algorithm for scalable inference and learning. Our algorithm introduces a recognition model to represent approximate posterior distributions, and that acts as a stochastic encoder of the data. We develop stochastic back-propagation -- rules for back-propagation through stochastic variables -- and use this to develop an algorithm that allows for joint optimisation of the parameters of both the generative and recognition model. We demonstrate on several real-world data sets that the model generates realistic samples, provides accurate imputations of missing data and is a useful tool for high-dimensional data visualisation.},
	urldate = {2022-01-10},
	journal = {arXiv:1401.4082 [cs, stat]},
	author = {Rezende, Danilo Jimenez and Mohamed, Shakir and Wierstra, Daan},
	month = may,
	year = {2014},
	note = {arXiv: 1401.4082},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Statistics - Computation, Statistics - Machine Learning, Statistics - Methodology},
	file = {arXiv Fulltext PDF:/home/ludvig/Zotero/storage/DIQHYCH9/Rezende et al. - 2014 - Stochastic Backpropagation and Approximate Inferen.pdf:application/pdf;arXiv.org Snapshot:/home/ludvig/Zotero/storage/K77TXJZA/1401.html:text/html},
}

@article{rezende_stochastic_2014-1,
	title = {Stochastic {Backpropagation} and {Approximate} {Inference} in {Deep} {Generative} {Models}},
	url = {http://arxiv.org/abs/1401.4082},
	abstract = {We marry ideas from deep neural networks and approximate Bayesian inference to derive a generalised class of deep, directed generative models, endowed with a new algorithm for scalable inference and learning. Our algorithm introduces a recognition model to represent an approximate posterior distribution and uses this for optimisation of a variational lower bound. We develop stochastic backpropagation – rules for gradient backpropagation through stochastic variables – and derive an algorithm that allows for joint optimisation of the parameters of both the generative and recognition models. We demonstrate on several real-world data sets that by using stochastic backpropagation and variational inference, we obtain models that are able to generate realistic samples of data, allow for accurate imputations of missing data, and provide a useful tool for high-dimensional data visualisation.},
	language = {en},
	urldate = {2022-01-10},
	journal = {arXiv:1401.4082 [cs, stat]},
	author = {Rezende, Danilo Jimenez and Mohamed, Shakir and Wierstra, Daan},
	month = may,
	year = {2014},
	note = {arXiv: 1401.4082},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Statistics - Computation, Statistics - Machine Learning, Statistics - Methodology},
	file = {Rezende et al. - 2014 - Stochastic Backpropagation and Approximate Inferen.pdf:/home/ludvig/Zotero/storage/R5SLJF48/Rezende et al. - 2014 - Stochastic Backpropagation and Approximate Inferen.pdf:application/pdf},
}

@inproceedings{ritter_scalable_2018,
	title = {A {Scalable} {Laplace} {Approximation} for {Neural} {Networks}},
	url = {https://openreview.net/forum?id=Skdvd2xAZ},
	abstract = {We construct a Kronecker factored Laplace approximation for neural networks that leads to an efficient matrix normal distribution over the weights.},
	language = {en},
	urldate = {2022-01-10},
	author = {Ritter, Hippolyt and Botev, Aleksandar and Barber, David},
	month = feb,
	year = {2018},
	file = {Full Text PDF:/home/ludvig/Zotero/storage/5LXSQGZ6/Ritter et al. - 2018 - A Scalable Laplace Approximation for Neural Networ.pdf:application/pdf;Snapshot:/home/ludvig/Zotero/storage/EE3I7R7B/forum.html:text/html},
}
