
@inproceedings{blundell_weight_2015,
	address = {Lille, France},
	series = {Proceedings of {Machine} {Learning} {Research}},
	title = {Weight {Uncertainty} in {Neural} {Network}},
	volume = {37},
	url = {http://proceedings.mlr.press/v37/blundell15.html},
	abstract = {We introduce a new, efficient, principled and backpropagation-compatible algorithm for learning a probability distribution on the weights of a neural network, called Bayes by Backprop. It regularises the weights by minimising a compression cost, known as the variational free energy or the expected lower bound on the marginal likelihood. We show that this principled kind of regularisation yields comparable performance to dropout on MNIST classification. We then demonstrate how the learnt uncertainty in the weights can be used to improve generalisation in non-linear regression problems, and how this weight uncertainty can be used to drive the exploration-exploitation trade-off in reinforcement learning.},
	booktitle = {Proceedings of the 32nd {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Blundell, Charles and Cornebise, Julien and Kavukcuoglu, Koray and Wierstra, Daan},
	editor = {Bach, Francis and Blei, David},
	month = jul,
	year = {2015},
	pages = {1613--1622},
}

@article{sun_functional_2019,
	title = {Functional {Variational} {Bayesian} {Neural} {Networks}},
	url = {http://arxiv.org/abs/1903.05779},
	abstract = {Variational Bayesian neural networks (BNNs) perform variational inference over weights, but it is difficult to specify meaningful priors and approximate posteriors in a high-dimensional weight space. We introduce functional variational Bayesian neural networks (fBNNs), which maximize an Evidence Lower BOund (ELBO) defined directly on stochastic processes, i.e. distributions over functions. We prove that the KL divergence between stochastic processes equals the supremum of marginal KL divergences over all finite sets of inputs. Based on this, we introduce a practical training objective which approximates the functional ELBO using finite measurement sets and the spectral Stein gradient estimator. With fBNNs, we can specify priors entailing rich structures, including Gaussian processes and implicit stochastic processes. Empirically, we find fBNNs extrapolate well using various structured priors, provide reliable uncertainty estimates, and scale to large datasets.},
	urldate = {2021-06-27},
	journal = {arXiv:1903.05779 [cs, stat]},
	author = {Sun, Shengyang and Zhang, Guodong and Shi, Jiaxin and Grosse, Roger},
	month = mar,
	year = {2019},
	note = {arXiv: 1903.05779},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/ludvig/Zotero/storage/7JMKZBL9/Sun et al. - 2019 - Functional Variational Bayesian Neural Networks.pdf:application/pdf;arXiv.org Snapshot:/Users/ludvig/Zotero/storage/WHPGS7US/1903.html:text/html},
}

@article{osband_generalization_2016,
	title = {Generalization and {Exploration} via {Randomized} {Value} {Functions}},
	url = {http://arxiv.org/abs/1402.0635},
	abstract = {We propose randomized least-squares value iteration (RLSVI) -- a new reinforcement learning algorithm designed to explore and generalize efficiently via linearly parameterized value functions. We explain why versions of least-squares value iteration that use Boltzmann or epsilon-greedy exploration can be highly inefficient, and we present computational results that demonstrate dramatic efficiency gains enjoyed by RLSVI. Further, we establish an upper bound on the expected regret of RLSVI that demonstrates near-optimality in a tabula rasa learning context. More broadly, our results suggest that randomized value functions offer a promising approach to tackling a critical challenge in reinforcement learning: synthesizing efficient exploration and effective generalization.},
	urldate = {2021-12-10},
	journal = {arXiv:1402.0635 [cs, stat]},
	author = {Osband, Ian and Van Roy, Benjamin and Wen, Zheng},
	month = feb,
	year = {2016},
	note = {arXiv: 1402.0635},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Artificial Intelligence, Electrical Engineering and Systems Science - Systems and Control},
	file = {arXiv Fulltext PDF:/Users/ludvig/Zotero/storage/I5ITSDEV/Osband et al. - 2016 - Generalization and Exploration via Randomized Valu.pdf:application/pdf;arXiv.org Snapshot:/Users/ludvig/Zotero/storage/2ZJ8WLUK/1402.html:text/html},
}

@article{fortunato_noisy_2019,
	title = {Noisy {Networks} for {Exploration}},
	url = {http://arxiv.org/abs/1706.10295},
	abstract = {We introduce NoisyNet, a deep reinforcement learning agent with parametric noise added to its weights, and show that the induced stochasticity of the agent’s policy can be used to aid efﬁcient exploration. The parameters of the noise are learned with gradient descent along with the remaining network weights. NoisyNet is straightforward to implement and adds little computational overhead. We ﬁnd that replacing the conventional exploration heuristics for A3C, DQN and Dueling agents (entropy reward and -greedy respectively) with NoisyNet yields substantially higher scores for a wide range of Atari games, in some cases advancing the agent from sub to super-human performance.},
	language = {en},
	urldate = {2021-12-03},
	journal = {arXiv:1706.10295 [cs, stat]},
	author = {Fortunato, Meire and Azar, Mohammad Gheshlaghi and Piot, Bilal and Menick, Jacob and Osband, Ian and Graves, Alex and Mnih, Vlad and Munos, Remi and Hassabis, Demis and Pietquin, Olivier and Blundell, Charles and Legg, Shane},
	month = jul,
	year = {2019},
	note = {arXiv: 1706.10295},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {Fortunato et al. - 2019 - Noisy Networks for Exploration.pdf:/Users/ludvig/Zotero/storage/M6DMZ8JL/Fortunato et al. - 2019 - Noisy Networks for Exploration.pdf:application/pdf},
}

@article{shi_spectral_2018,
	title = {A {Spectral} {Approach} to {Gradient} {Estimation} for {Implicit} {Distributions}},
	url = {https://arxiv.org/abs/1806.02925v1},
	abstract = {Recently there have been increasing interests in learning and inference with implicit distributions (i.e., distributions without tractable densities). To this end, we develop a gradient estimator for implicit distributions based on Stein's identity and a spectral decomposition of kernel operators, where the eigenfunctions are approximated by the Nystr{\textbackslash}"om method. Unlike the previous works that only provide estimates at the sample points, our approach directly estimates the gradient function, thus allows for a simple and principled out-of-sample extension. We provide theoretical results on the error bound of the estimator and discuss the bias-variance tradeoff in practice. The effectiveness of our method is demonstrated by applications to gradient-free Hamiltonian Monte Carlo and variational inference with implicit distributions. Finally, we discuss the intuition behind the estimator by drawing connections between the Nystr{\textbackslash}"om method and kernel PCA, which indicates that the estimator can automatically adapt to the geometry of the underlying distribution.},
	language = {en},
	urldate = {2021-11-08},
	author = {Shi, Jiaxin and Sun, Shengyang and Zhu, Jun},
	month = jun,
	year = {2018},
	file = {Full Text PDF:/Users/ludvig/Zotero/storage/JY6MCQC6/Shi et al. - 2018 - A Spectral Approach to Gradient Estimation for Imp.pdf:application/pdf;Snapshot:/Users/ludvig/Zotero/storage/L3BKL58T/1806.html:text/html},
}

@book{wiering_reinforcement_2012,
	address = {Berlin, Heidelberg},
	series = {Adaptation, {Learning}, and {Optimization}},
	title = {Reinforcement {Learning}},
	volume = {12},
	isbn = {978-3-642-27644-6 978-3-642-27645-3},
	url = {http://link.springer.com/10.1007/978-3-642-27645-3},
	language = {en},
	urldate = {2021-11-05},
	publisher = {Springer Berlin Heidelberg},
	editor = {Wiering, Marco and van Otterlo, Martijn},
	year = {2012},
	doi = {10.1007/978-3-642-27645-3},
	file = {Wiering and van Otterlo - 2012 - Reinforcement Learning.pdf:/Users/ludvig/Zotero/storage/YBSDDI8M/Wiering and van Otterlo - 2012 - Reinforcement Learning.pdf:application/pdf},
}

@article{ghavamzadeh_bayesian_2015,
	title = {Bayesian {Reinforcement} {Learning}: {A} {Survey}},
	volume = {8},
	issn = {1935-8237, 1935-8245},
	shorttitle = {Bayesian {Reinforcement} {Learning}},
	url = {http://arxiv.org/abs/1609.04436},
	doi = {10.1561/2200000049},
	abstract = {Bayesian methods for machine learning have been widely investigated, yielding principled methods for incorporating prior information into inference algorithms. In this survey, we provide an in-depth review of the role of Bayesian methods for the reinforcement learning (RL) paradigm. The major incentives for incorporating Bayesian reasoning in RL are: 1) it provides an elegant approach to action-selection (exploration/exploitation) as a function of the uncertainty in learning; and 2) it provides a machinery to incorporate prior knowledge into the algorithms. We first discuss models and methods for Bayesian inference in the simple single-step Bandit model. We then review the extensive recent literature on Bayesian methods for model-based RL, where prior information can be expressed on the parameters of the Markov model. We also present Bayesian methods for model-free RL, where priors are expressed over the value function or policy class. The objective of the paper is to provide a comprehensive survey on Bayesian RL algorithms and their theoretical and empirical properties.},
	language = {en},
	number = {5-6},
	urldate = {2021-11-05},
	journal = {Foundations and Trends® in Machine Learning},
	author = {Ghavamzadeh, Mohammad and Mannor, Shie and Pineau, Joelle and Tamar, Aviv},
	year = {2015},
	note = {arXiv: 1609.04436},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Artificial Intelligence},
	pages = {359--483},
	file = {Ghavamzadeh et al. - 2015 - Bayesian Reinforcement Learning A Survey.pdf:/Users/ludvig/Zotero/storage/GCU5BZM7/Ghavamzadeh et al. - 2015 - Bayesian Reinforcement Learning A Survey.pdf:application/pdf},
}

@article{osband_deep_2019,
	title = {Deep {Exploration} via {Randomized} {Value} {Functions}},
	url = {http://arxiv.org/abs/1703.07608},
	abstract = {We study the use of randomized value functions to guide deep exploration in reinforcement learning. This oﬀers an elegant means for synthesizing statistically and computationally eﬃcient exploration with common practical approaches to value function learning. We present several reinforcement learning algorithms that leverage randomized value functions and demonstrate their eﬃcacy through computational studies. We also prove a regret bound that establishes statistical eﬃciency with a tabular representation.},
	language = {en},
	urldate = {2021-10-28},
	journal = {arXiv:1703.07608 [cs, stat]},
	author = {Osband, Ian and Van Roy, Benjamin and Russo, Daniel and Wen, Zheng},
	month = sep,
	year = {2019},
	note = {arXiv: 1703.07608},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Artificial Intelligence},
	file = {Osband et al. - 2019 - Deep Exploration via Randomized Value Functions.pdf:/Users/ludvig/Zotero/storage/7UZW8V2P/Osband et al. - 2019 - Deep Exploration via Randomized Value Functions.pdf:application/pdf},
}

@article{osband_randomized_2018,
	title = {Randomized {Prior} {Functions} for {Deep} {Reinforcement} {Learning}},
	url = {http://arxiv.org/abs/1806.03335},
	abstract = {Dealing with uncertainty is essential for eﬃcient reinforcement learning. There is a growing literature on uncertainty estimation for deep learning from ﬁxed datasets, but many of the most popular approaches are poorlysuited to sequential decision problems. Other methods, such as bootstrap sampling, have no mechanism for uncertainty that does not come from the observed data. We highlight why this can be a crucial shortcoming and propose a simple remedy through addition of a randomized untrainable ‘prior’ network to each ensemble member. We prove that this approach is eﬃcient with linear representations, provide simple illustrations of its eﬃcacy with nonlinear representations and show that this approach scales to large-scale problems far better than previous attempts.},
	language = {en},
	urldate = {2021-10-28},
	journal = {arXiv:1806.03335 [cs, stat]},
	author = {Osband, Ian and Aslanides, John and Cassirer, Albin},
	month = nov,
	year = {2018},
	note = {arXiv: 1806.03335},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Artificial Intelligence},
	file = {Osband et al. - 2018 - Randomized Prior Functions for Deep Reinforcement .pdf:/Users/ludvig/Zotero/storage/QGX8T5FR/Osband et al. - 2018 - Randomized Prior Functions for Deep Reinforcement .pdf:application/pdf},
}

@article{riquelme_deep_2018,
	title = {{DEEP} {BAYESIAN} {BANDITS} {SHOWDOWN}},
	abstract = {Recent advances in deep reinforcement learning have made signiﬁcant strides in performance on applications such as Go and Atari games. However, developing practical methods to balance exploration and exploitation in complex domains remains largely unsolved. Thompson Sampling and its extension to reinforcement learning provide an elegant approach to exploration that only requires access to posterior samples of the model. At the same time, advances in approximate Bayesian methods have made posterior approximation for ﬂexible neural network models practical. Thus, it is attractive to consider approximate Bayesian neural networks in a Thompson Sampling framework. To understand the impact of using an approximate posterior on Thompson Sampling, we benchmark well-established and recently developed methods for approximate posterior sampling combined with Thompson Sampling over a series of contextual bandit problems. We found that many approaches that have been successful in the supervised learning setting underperformed in the sequential decision-making scenario. In particular, we highlight the challenge of adapting slowly converging uncertainty estimates to the online setting.},
	language = {en},
	author = {Riquelme, Carlos and Tucker, George and Snoek, Jasper},
	year = {2018},
	pages = {27},
	file = {Riquelme et al. - 2018 - DEEP BAYESIAN BANDITS SHOWDOWN.pdf:/Users/ludvig/Zotero/storage/BSYYB96Z/Riquelme et al. - 2018 - DEEP BAYESIAN BANDITS SHOWDOWN.pdf:application/pdf},
}

@article{fellows_bayesian_2021,
	title = {Bayesian {Bellman} {Operators}},
	url = {http://arxiv.org/abs/2106.05012},
	abstract = {We introduce a novel perspective on Bayesian reinforcement learning (RL); whereas existing approaches infer a posterior over the transition distribution or Q-function, we characterise the uncertainty in the Bellman operator. Our Bayesian Bellman operator (BBO) framework is motivated by the insight that when bootstrapping is introduced, model-free approaches actually infer a posterior over Bellman operators, not value functions. In this paper, we use BBO to provide a rigorous theoretical analysis of model-free Bayesian RL to better understand its relationship to established frequentist RL methodologies. We prove that Bayesian solutions are consistent with frequentist RL solutions, even when approximate inference is used, and derive conditions for which convergence properties hold. Empirically, we demonstrate that algorithms derived from the BBO framework have sophisticated deep exploration properties that enable them to solve continuous control tasks at which state-of-the-art regularised actor-critic algorithms fail catastrophically.},
	language = {en},
	urldate = {2021-10-28},
	journal = {arXiv:2106.05012 [cs]},
	author = {Fellows, Matthew and Hartikainen, Kristian and Whiteson, Shimon},
	month = jun,
	year = {2021},
	note = {arXiv: 2106.05012},
	keywords = {Computer Science - Machine Learning},
	file = {Fellows et al. - 2021 - Bayesian Bellman Operators.pdf:/Users/ludvig/Zotero/storage/874CC8RB/Fellows et al. - 2021 - Bayesian Bellman Operators.pdf:application/pdf},
}

@article{dearden_bayesian_nodate,
	title = {Bayesian {Q}-{Learning}},
	abstract = {A central problem in learning in complex environments is balancing exploration of untested actions against exploitation of actions that are known to be good. The beneﬁt of exploration can be estimated using the classical notion of Value of Information—the expected improvement in future decision quality that might arise from the information acquired by exploration. Estimating this quantity requires an assessment of the agent’s uncertainty about its current value estimates for states. In this paper, we adopt a Bayesian approach to maintaining this uncertain information. We extend Watkins’ Q-learning by maintaining and propagating probability distributions over the Q-values. These distributions are used to compute a myopic approximation to the value of information for each action and hence to select the action that best balances exploration and exploitation. We establish the convergence properties of our algorithm and show experimentally that it can exhibit substantial improvements over other well-known model-free exploration strategies.},
	language = {en},
	author = {Dearden, Richard and Friedman, Nir and Russell, Stuart},
	pages = {8},
	file = {Dearden et al. - Bayesian Q-Learning.pdf:/Users/ludvig/Zotero/storage/7NGS3ZD7/Dearden et al. - Bayesian Q-Learning.pdf:application/pdf},
}

@article{odonoghue_uncertainty_2018,
	title = {The {Uncertainty} {Bellman} {Equation} and {Exploration}},
	url = {http://arxiv.org/abs/1709.05380},
	abstract = {We consider the exploration/exploitation problem in reinforcement learning. For exploitation, it is well known that the Bellman equation connects the value at any time-step to the expected value at subsequent time-steps. In this paper we consider a similar uncertainty Bellman equation (UBE), which connects the uncertainty at any time-step to the expected uncertainties at subsequent time-steps, thereby extending the potential exploratory beneﬁt of a policy beyond individual time-steps. We prove that the unique ﬁxed point of the UBE yields an upper bound on the variance of the posterior distribution of the Q-values induced by any policy. This bound can be much tighter than traditional count-based bonuses that compound standard deviation rather than variance. Importantly, and unlike several existing approaches to optimism, this method scales naturally to large systems with complex generalization. Substituting our UBE-exploration strategy for -greedy improves DQN performance on 51 out of 57 games in the Atari suite.},
	language = {en},
	urldate = {2021-10-28},
	journal = {arXiv:1709.05380 [cs, math, stat]},
	author = {O'Donoghue, Brendan and Osband, Ian and Munos, Remi and Mnih, Volodymyr},
	month = oct,
	year = {2018},
	note = {arXiv: 1709.05380},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Artificial Intelligence, Mathematics - Optimization and Control},
	file = {O'Donoghue et al. - 2018 - The Uncertainty Bellman Equation and Exploration.pdf:/Users/ludvig/Zotero/storage/5MWSY3TP/O'Donoghue et al. - 2018 - The Uncertainty Bellman Equation and Exploration.pdf:application/pdf},
}

@inproceedings{osband_more_2013,
	title = {({More}) {Efficient} {Reinforcement} {Learning} via {Posterior} {Sampling}},
	volume = {26},
	url = {https://proceedings.neurips.cc/paper/2013/hash/6a5889bb0190d0211a991f47bb19a777-Abstract.html},
	urldate = {2021-10-28},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Osband, Ian and Russo, Daniel and Van Roy, Benjamin},
	year = {2013},
	file = {Full Text PDF:/Users/ludvig/Zotero/storage/PGSG2ZT4/Osband et al. - 2013 - (More) Efficient Reinforcement Learning via Poster.pdf:application/pdf},
}

@article{hessel_rainbow_2017,
	title = {Rainbow: {Combining} {Improvements} in {Deep} {Reinforcement} {Learning}},
	shorttitle = {Rainbow},
	url = {http://arxiv.org/abs/1710.02298},
	abstract = {The deep reinforcement learning community has made several independent improvements to the DQN algorithm. However, it is unclear which of these extensions are complementary and can be fruitfully combined. This paper examines six extensions to the DQN algorithm and empirically studies their combination. Our experiments show that the combination provides state-of-the-art performance on the Atari 2600 benchmark, both in terms of data efficiency and final performance. We also provide results from a detailed ablation study that shows the contribution of each component to overall performance.},
	urldate = {2021-12-14},
	journal = {arXiv:1710.02298 [cs]},
	author = {Hessel, Matteo and Modayil, Joseph and van Hasselt, Hado and Schaul, Tom and Ostrovski, Georg and Dabney, Will and Horgan, Dan and Piot, Bilal and Azar, Mohammad and Silver, David},
	month = oct,
	year = {2017},
	note = {arXiv: 1710.02298},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/ludvig/Zotero/storage/QXSL4IM3/Hessel et al. - 2017 - Rainbow Combining Improvements in Deep Reinforcem.pdf:application/pdf;arXiv.org Snapshot:/Users/ludvig/Zotero/storage/5JWSJVGK/1710.html:text/html},
}

@article{wang_dueling_2016,
	title = {Dueling {Network} {Architectures} for {Deep} {Reinforcement} {Learning}},
	url = {http://arxiv.org/abs/1511.06581},
	abstract = {In recent years there have been many successes of using deep representations in reinforcement learning. Still, many of these applications use conventional architectures, such as convolutional networks, LSTMs, or auto-encoders. In this paper, we present a new neural network architecture for model-free reinforcement learning. Our dueling network represents two separate estimators: one for the state value function and one for the state-dependent action advantage function. The main benefit of this factoring is to generalize learning across actions without imposing any change to the underlying reinforcement learning algorithm. Our results show that this architecture leads to better policy evaluation in the presence of many similar-valued actions. Moreover, the dueling architecture enables our RL agent to outperform the state-of-the-art on the Atari 2600 domain.},
	urldate = {2021-12-14},
	journal = {arXiv:1511.06581 [cs]},
	author = {Wang, Ziyu and Schaul, Tom and Hessel, Matteo and van Hasselt, Hado and Lanctot, Marc and de Freitas, Nando},
	month = apr,
	year = {2016},
	note = {arXiv: 1511.06581},
	keywords = {Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/ludvig/Zotero/storage/BZ8ZVAPF/Wang et al. - 2016 - Dueling Network Architectures for Deep Reinforceme.pdf:application/pdf;arXiv.org Snapshot:/Users/ludvig/Zotero/storage/LDULE24P/1511.html:text/html},
}

@article{van_hasselt_deep_2015,
	title = {Deep {Reinforcement} {Learning} with {Double} {Q}-learning},
	url = {http://arxiv.org/abs/1509.06461},
	abstract = {The popular Q-learning algorithm is known to overestimate action values under certain conditions. It was not previously known whether, in practice, such overestimations are common, whether they harm performance, and whether they can generally be prevented. In this paper, we answer all these questions affirmatively. In particular, we first show that the recent DQN algorithm, which combines Q-learning with a deep neural network, suffers from substantial overestimations in some games in the Atari 2600 domain. We then show that the idea behind the Double Q-learning algorithm, which was introduced in a tabular setting, can be generalized to work with large-scale function approximation. We propose a specific adaptation to the DQN algorithm and show that the resulting algorithm not only reduces the observed overestimations, as hypothesized, but that this also leads to much better performance on several games.},
	urldate = {2021-12-14},
	journal = {arXiv:1509.06461 [cs]},
	author = {van Hasselt, Hado and Guez, Arthur and Silver, David},
	month = dec,
	year = {2015},
	note = {arXiv: 1509.06461},
	keywords = {Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/ludvig/Zotero/storage/C5Z2JNDA/van Hasselt et al. - 2015 - Deep Reinforcement Learning with Double Q-learning.pdf:application/pdf;arXiv.org Snapshot:/Users/ludvig/Zotero/storage/X27F4QMG/1509.html:text/html},
}

@article{mnih_asynchronous_2016,
	title = {Asynchronous {Methods} for {Deep} {Reinforcement} {Learning}},
	url = {http://arxiv.org/abs/1602.01783},
	abstract = {We propose a conceptually simple and lightweight framework for deep reinforcement learning that uses asynchronous gradient descent for optimization of deep neural network controllers. We present asynchronous variants of four standard reinforcement learning algorithms and show that parallel actor-learners have a stabilizing effect on training allowing all four methods to successfully train neural network controllers. The best performing method, an asynchronous variant of actor-critic, surpasses the current state-of-the-art on the Atari domain while training for half the time on a single multi-core CPU instead of a GPU. Furthermore, we show that asynchronous actor-critic succeeds on a wide variety of continuous motor control problems as well as on a new task of navigating random 3D mazes using a visual input.},
	urldate = {2021-12-14},
	journal = {arXiv:1602.01783 [cs]},
	author = {Mnih, Volodymyr and Badia, Adrià Puigdomènech and Mirza, Mehdi and Graves, Alex and Lillicrap, Timothy P. and Harley, Tim and Silver, David and Kavukcuoglu, Koray},
	month = jun,
	year = {2016},
	note = {arXiv: 1602.01783},
	keywords = {Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/ludvig/Zotero/storage/842IUBN8/Mnih et al. - 2016 - Asynchronous Methods for Deep Reinforcement Learni.pdf:application/pdf;arXiv.org Snapshot:/Users/ludvig/Zotero/storage/J8WPHCY7/1602.html:text/html},
}

@article{schaul_prioritized_2016,
	title = {Prioritized {Experience} {Replay}},
	url = {http://arxiv.org/abs/1511.05952},
	abstract = {Experience replay lets online reinforcement learning agents remember and reuse experiences from the past. In prior work, experience transitions were uniformly sampled from a replay memory. However, this approach simply replays transitions at the same frequency that they were originally experienced, regardless of their significance. In this paper we develop a framework for prioritizing experience, so as to replay important transitions more frequently, and therefore learn more efficiently. We use prioritized experience replay in Deep Q-Networks (DQN), a reinforcement learning algorithm that achieved human-level performance across many Atari games. DQN with prioritized experience replay achieves a new state-of-the-art, outperforming DQN with uniform replay on 41 out of 49 games.},
	urldate = {2021-12-14},
	journal = {arXiv:1511.05952 [cs]},
	author = {Schaul, Tom and Quan, John and Antonoglou, Ioannis and Silver, David},
	month = feb,
	year = {2016},
	note = {arXiv: 1511.05952},
	keywords = {Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/ludvig/Zotero/storage/ANALD9C5/Schaul et al. - 2016 - Prioritized Experience Replay.pdf:application/pdf;arXiv.org Snapshot:/Users/ludvig/Zotero/storage/XFMT6X8U/1511.html:text/html},
}

@article{bellemare_distributional_2017,
	title = {A {Distributional} {Perspective} on {Reinforcement} {Learning}},
	url = {http://arxiv.org/abs/1707.06887},
	abstract = {In this paper we argue for the fundamental importance of the value distribution: the distribution of the random return received by a reinforcement learning agent. This is in contrast to the common approach to reinforcement learning which models the expectation of this return, or value. Although there is an established body of literature studying the value distribution, thus far it has always been used for a specific purpose such as implementing risk-aware behaviour. We begin with theoretical results in both the policy evaluation and control settings, exposing a significant distributional instability in the latter. We then use the distributional perspective to design a new algorithm which applies Bellman's equation to the learning of approximate value distributions. We evaluate our algorithm using the suite of games from the Arcade Learning Environment. We obtain both state-of-the-art results and anecdotal evidence demonstrating the importance of the value distribution in approximate reinforcement learning. Finally, we combine theoretical and empirical evidence to highlight the ways in which the value distribution impacts learning in the approximate setting.},
	urldate = {2021-12-14},
	journal = {arXiv:1707.06887 [cs, stat]},
	author = {Bellemare, Marc G. and Dabney, Will and Munos, Rémi},
	month = jul,
	year = {2017},
	note = {arXiv: 1707.06887},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/ludvig/Zotero/storage/2J4F3TB2/Bellemare et al. - 2017 - A Distributional Perspective on Reinforcement Lear.pdf:application/pdf;arXiv.org Snapshot:/Users/ludvig/Zotero/storage/UKFPP77R/1707.html:text/html},
}
